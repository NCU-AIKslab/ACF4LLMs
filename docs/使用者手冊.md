# 使用者手冊

**Agentic Compression Framework**

版本：1.0.0
最後更新：2026-01-26

---

## 目錄

- [0. 開始之前](#0-開始之前)
- [1. 快速上手](#1-快速上手)
- [2. 核心使用概念](#2-核心使用概念)
- [3. 使用 CLI](#3-使用-cli)
- [4. 使用 Web UI](#4-使用-web-ui)
- [5. 使用 REST API](#5-使用-rest-api)
- [6. 常用情境教學](#6-常用情境教學)
- [7. 輸出結果解讀](#7-輸出結果解讀)
- [8. 故障排除](#8-故障排除)
- [9. FAQ](#9-faq)
- [10. 附錄](#10-附錄)

---

## 0. 開始之前

### 0.1 手冊目的與適用對象

本手冊適用於需要壓縮大型語言模型（LLM）的使用者，包括：
- 機器學習工程師
- 資料科學家
- AI 應用開發者
- 研究人員

無論您是否具有深度學習背景，本手冊都將引導您完成模型壓縮任務。

### 0.2 系統能力與限制

**系統能力**：
- 自動壓縮 HuggingFace 上的 LLM 模型
- 支援多種壓縮方法（量化、剪枝、LoRA 等）
- 多目標最佳化（準確度、速度、大小、能耗）
- 自動產生 Pareto 最佳解集合
- 提供互動式視覺化結果

**系統限制**：
- 需要 NVIDIA GPU（建議 8GB+ VRAM）
- 需要 OpenAI API Key（用於 AI 決策）
- 大型模型（70B+）需要大量 VRAM 或使用 QLoRA
- 部分模型需要 HuggingFace 存取權限

### 0.3 名詞解釋

| 名詞 | 說明 |
|------|------|
| **Pareto 前沿** | 在多目標最佳化中，沒有任何解能同時在所有目標上勝過的解集合。例如：模型 A 準確度最高但較慢，模型 B 最快但準確度稍低，兩者都是 Pareto 最佳解。 |
| **量化（Quantization）** | 將模型權重從 32-bit 浮點數壓縮為 4-bit 或 8-bit 整數，大幅減少模型大小。 |
| **剪枝（Pruning）** | 移除模型中不重要的權重或神經元，減少計算量。 |
| **LoRA** | Low-Rank Adaptation，一種高效的微調方法，只訓練少量額外參數。 |
| **QLoRA** | 結合 4-bit 量化與 LoRA，可在記憶體有限的情況下微調大型模型。 |
| **Episode** | 一次壓縮嘗試，包含策略選擇、執行壓縮、評測結果的完整循環。 |
| **VRAM** | GPU 視訊記憶體，決定可處理的模型大小。 |

### 0.4 系統需求

**硬體需求**：
| 項目 | 最低需求 | 建議配置 |
|------|----------|----------|
| CPU | 4 核心 | 8 核心以上 |
| RAM | 16 GB | 32 GB |
| GPU | NVIDIA 8GB VRAM | NVIDIA 24GB+ VRAM |
| 磁碟 | 50 GB | 200 GB SSD |

**軟體需求**：
- 作業系統：Linux（建議）、Windows 10/11、macOS
- Python 3.10 以上
- CUDA Toolkit 11.8+（GPU 加速需要）

---

## 1. 快速上手

### 1.1 安裝與設定

**步驟 1：建立環境**

```bash
# 使用 conda 建立虛擬環境
conda create -n greenai python=3.10
conda activate greenai

# 安裝依賴
pip install -r requirements.txt
```

**步驟 2：設定 API Key**

```bash
# 設定 OpenAI API Key（必要）
export OPENAI_API_KEY=sk-your-api-key-here

# 設定 HuggingFace Token（選用，存取受限模型需要）
export HF_TOKEN=hf_your_token_here
```

**步驟 3：驗證安裝**

```bash
# 確認 GPU 可用
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

### 1.2 第一個壓縮任務

讓我們壓縮一個小型模型 `gpt2`：

```bash
python scripts/run_pipeline.py \
    --model gpt2 \
    --dataset gsm8k \
    --episodes 3
```

**執行過程說明**：
1. 系統會自動推論 gpt2 的規格
2. GPT-4o 協調器開始決定壓縮策略
3. 每個 episode 會嘗試一種壓縮方法
4. 評測壓縮後模型的準確度、速度、大小
5. 更新 Pareto 前沿

### 1.3 查看結果

壓縮完成後，結果會儲存在 `data/experiments/` 目錄：

```
data/experiments/gpt2_gsm8k_20260126_143022/
├── model_spec.json           # 模型規格
├── pareto_frontier.json      # Pareto 最佳解
├── final_results.json        # 最終結果摘要
├── pareto_visualization.html # 互動式圖表（用瀏覽器開啟）
└── episode_001/              # 各 episode 詳細資料
    ├── strategy.json
    └── results.json
```

**查看視覺化圖表**：
```bash
# 用瀏覽器開啟
xdg-open data/experiments/gpt2_gsm8k_20260126_143022/pareto_visualization.html
```

### 1.4 常見新手錯誤

| 錯誤 | 原因 | 解決方法 |
|------|------|----------|
| `OPENAI_API_KEY not set` | 未設定環境變數 | `export OPENAI_API_KEY=sk-...` |
| `CUDA out of memory` | GPU 記憶體不足 | 使用較小模型或降低 batch size |
| `Model not found` | 模型名稱錯誤 | 確認 HuggingFace 上的完整模型名稱 |
| `Access denied` | 需要模型存取權限 | 到 HuggingFace 申請並設定 HF_TOKEN |

---

## 2. 核心使用概念

### 2.1 多目標最佳化

模型壓縮涉及多個相互矛盾的目標：

| 目標 | 說明 | 優化方向 |
|------|------|----------|
| **準確度** | 模型在基準測試的表現 | 越高越好 |
| **延遲** | 推論一個請求的時間 | 越低越好 |
| **記憶體** | 執行時的 GPU 記憶體使用 | 越低越好 |
| **模型大小** | 儲存所需的磁碟空間 | 越低越好 |
| **碳排放** | 壓縮過程的碳排放量 | 越低越好 |

系統會自動找出這些目標之間的最佳平衡點。

### 2.2 壓縮方法說明

#### 量化方法

| 方法 | bit 數 | 壓縮率 | 準確度損失 | 特點 |
|------|--------|--------|------------|------|
| **GPTQ** | 4/8 | ~4x | 低 | 最常用，需要校準資料 |
| **AutoRound** | 4/8 | ~4x | 低 | Intel 開發，準確度較佳 |
| **AWQ** | 4 | ~4x | 低 | 活化感知，速度快 |
| **INT8** | 8 | ~2x | 極低 | 動態量化，最簡單 |

#### 微調方法

| 方法 | 適用情境 | VRAM 需求 |
|------|----------|-----------|
| **LoRA** | 恢復量化損失 | 中 |
| **QLoRA** | 大型模型微調 | 低 |
| **ASVD** | 低秩分解壓縮 | 中 |

#### 剪枝方法

| 方法 | 說明 |
|------|------|
| **Magnitude** | 移除絕對值小的權重 |
| **Structured** | 移除整個神經元或通道 |

### 2.3 評測基準說明

| 基準 | 類型 | 說明 |
|------|------|------|
| **GSM8K** | 數學推理 | 小學數學應用題 |
| **CommonsenseQA** | 常識推理 | 常識問答 |
| **TruthfulQA** | 真實性 | 檢測幻覺/錯誤資訊 |
| **HumanEval** | 程式碼 | Python 程式碼生成 |
| **BIG-Bench Hard** | 綜合 | 困難推理任務集合 |
| **MMLU** | 知識 | 多領域知識測驗 |
| **HellaSwag** | 常識 | 情境完成任務 |
| **ARC** | 科學 | 小學科學問答 |
| **WinoGrande** | 共指消解 | 代詞消歧義 |

### 2.4 Pareto 前沿：如何選擇最佳模型

Pareto 前沿上的每個解都是「最佳」的，選擇取決於您的優先級：

**情境 1：準確度優先**
- 選擇 Pareto 前沿上準確度最高的解
- 通常是 8-bit 量化或 LoRA 恢復後的模型

**情境 2：速度優先**
- 選擇延遲最低的解
- 通常是 4-bit 量化 + 剪枝的組合

**情境 3：部署到邊緣裝置**
- 選擇模型大小最小的解
- 通常是積極的 4-bit 量化

**情境 4：平衡**
- 選擇位於 Pareto 前沿中間的解
- 在各指標間取得良好平衡

---

## 3. 使用 CLI

### 3.1 基本指令格式

```bash
python scripts/run_pipeline.py [命令] [選項]
```

### 3.2 參數說明

| 參數 | 簡寫 | 必填 | 說明 | 預設值 |
|------|------|------|------|--------|
| `--model` | `-m` | 是 | HuggingFace 模型名稱 | - |
| `--dataset` | `-d` | 是 | 評測資料集 | - |
| `--episodes` | `-e` | 否 | 最大嘗試次數 | 3 |
| `--budget` | `-b` | 否 | 時間預算（小時） | 2.0 |
| `--interactive` | `-i` | 否 | 互動模式 | False |
| `--output-dir` | `-o` | 否 | 輸出目錄 | `data/experiments` |
| `--show-spec` | - | 否 | 顯示模型規格後退出 | False |
| `--config` | `-c` | 否 | 設定檔路徑 | - |

### 3.3 互動模式

使用 `--interactive` 或 `-i` 開啟互動模式，可以看到更詳細的進度：

```bash
python scripts/run_pipeline.py -m gpt2 -d gsm8k -e 5 -i
```

**互動模式輸出範例**：
```
=== Episode 1/5 ===
[Coordinator] Analyzing Pareto frontier...
[Coordinator] Decision: quantization (method=gptq, bits=4)
[Coordinator] Reasoning: "GPTQ 4-bit 是起始點的好選擇，
               可以快速建立基準線..."
[Quantization] Applying GPTQ 4-bit quantization...
[Quantization] Compression ratio: 3.92x
[Evaluation] Running GSM8K benchmark...
[Evaluation] Accuracy: 0.42 | Latency: 23ms | Memory: 1.2GB
[Update] New Pareto solution found!

=== Episode 2/5 ===
...
```

### 3.4 分析歷史實驗

```bash
# 基本分析
python scripts/run_pipeline.py analyze data/experiments/EXPERIMENT_DIR

# 詳細分析
python scripts/run_pipeline.py analyze data/experiments/EXPERIMENT_DIR --format detailed

# JSON 輸出
python scripts/run_pipeline.py analyze data/experiments/EXPERIMENT_DIR --format json
```

**設定檔範例**（`config.json`）：
```json
{
  "model": "meta-llama/Meta-Llama-3-8B",
  "dataset": "gsm8k",
  "episodes": 10,
  "budget": 4.0,
  "compression_methods": ["gptq", "awq", "lora"],
  "constraints": {
    "min_accuracy": 0.9,
    "max_latency_ms": 50.0
  }
}
```

使用設定檔：
```bash
python scripts/run_pipeline.py --config config.json
```

---

## 4. 使用 Web UI

### 4.1 介面導覽

啟動 Web UI：
```bash
cd ui
npm install  # 首次執行
npm run dev
```

瀏覽器開啟：`http://localhost:3000`

**主要頁面**：
| 頁面 | 路徑 | 功能 |
|------|------|------|
| Dashboard | `/` | 系統狀態總覽 |
| Experiments | `/experiments` | 實驗列表 |
| New Experiment | `/new` | 建立新實驗 |
| Experiment Detail | `/experiments/{id}` | 實驗詳情 |

### 4.2 建立新實驗

1. 點擊側邊欄的「New Experiment」
2. 填寫表單：
   - **Model Name**：輸入 HuggingFace 模型名稱（如 `gpt2`）
   - **Dataset**：選擇評測資料集
   - **Max Episodes**：設定最大嘗試次數
3. 點擊「Start Compression」

### 4.3 即時進度監控

實驗執行中，可以在 Experiment Detail 頁面查看：
- 目前 Episode 進度
- 即時日誌輸出
- Coordinator 的決策理由
- 已找到的 Pareto 解

### 4.4 查看 Pareto 圖表

Experiment Detail 頁面包含互動式 Pareto 圖表：
- X 軸：選擇要比較的指標（如壓縮率）
- Y 軸：選擇另一個指標（如準確度）
- 每個點代表一個 Pareto 解
- 懸停顯示詳細資訊

### 4.5 下載壓縮模型

在 Experiment Detail 頁面：
1. 找到想要的 Pareto 解
2. 點擊「Download」按鈕
3. 下載 `.tar.gz` 壓縮檔
4. 解壓後可直接用於推論

---

## 5. 使用 REST API

### 5.1 API 端點一覽

啟動 API 服務：
```bash
uvicorn src.api.main:app --port 8000
```

API 文件：`http://localhost:8000/docs`

| 方法 | 端點 | 說明 |
|------|------|------|
| GET | `/` | API 資訊 |
| POST | `/compress` | 建立壓縮任務 |
| GET | `/jobs/{job_id}` | 查詢任務狀態 |
| GET | `/jobs/{job_id}/logs` | 取得任務日誌 |
| GET | `/jobs` | 列出所有任務 |
| GET | `/episodes/{job_id}` | 取得 Episode 歷史 |
| GET | `/experiments` | 列出所有實驗 |
| GET | `/experiments/{id}` | 取得實驗詳情 |
| POST | `/spec/infer` | 推論模型規格 |
| GET | `/pareto/{job_id}` | 取得 Pareto 前沿 |
| GET | `/methods` | 列出壓縮方法 |
| GET | `/benchmarks` | 列出評測基準 |
| GET | `/health` | 健康檢查 |
| GET | `/gpu` | GPU 狀態 |

### 5.2 建立壓縮任務

```bash
curl -X POST http://localhost:8000/compress \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "gpt2",
    "dataset": "gsm8k",
    "max_episodes": 5
  }'
```

**回應**：
```json
{
  "job_id": "abc123-def456-789",
  "status": "pending",
  "created_at": "2026-01-26T14:30:00",
  "progress": {
    "current_episode": 0,
    "max_episodes": 5
  }
}
```

### 5.3 查詢任務狀態

```bash
curl http://localhost:8000/jobs/abc123-def456-789
```

**回應**：
```json
{
  "job_id": "abc123-def456-789",
  "status": "running",
  "progress": {
    "current_episode": 3,
    "max_episodes": 5,
    "pareto_solutions": 2
  }
}
```

**狀態說明**：
| 狀態 | 說明 |
|------|------|
| `pending` | 等待執行 |
| `running` | 執行中 |
| `completed` | 完成 |
| `failed` | 失敗 |

### 5.4 取得 Pareto 前沿

```bash
curl http://localhost:8000/pareto/abc123-def456-789
```

**回應**：
```json
{
  "solutions": [
    {
      "strategy": {
        "method": "gptq",
        "bits": 4
      },
      "result": {
        "accuracy": 0.85,
        "latency_ms": 25.3,
        "compression_ratio": 3.92
      }
    }
  ]
}
```

---

## 6. 常用情境教學

### 6.1 快速壓縮小模型（gpt2）

適合：測試系統、學習使用

```bash
# 快速 3 episode 測試
python scripts/run_pipeline.py \
    --model gpt2 \
    --dataset gsm8k \
    --episodes 3
```

### 6.2 壓縮大型模型（Llama-8B）

適合：實際部署需求

```bash
# 需要 HuggingFace Token
export HF_TOKEN=hf_your_token_here

# 較多 episode 以找到更好的解
python scripts/run_pipeline.py \
    --model meta-llama/Meta-Llama-3-8B \
    --dataset gsm8k \
    --episodes 10 \
    --budget 4.0 \
    --interactive
```

**注意事項**：
- 需要 16GB+ GPU VRAM
- 執行時間較長
- 建議使用互動模式監控進度

### 6.3 多輪最佳化策略

如果第一次執行結果不理想，可以：

**策略 1：增加 Episode 數**
```bash
python scripts/run_pipeline.py -m MODEL -d DATASET -e 15
```

**策略 2：使用設定檔限制方法**
```json
{
  "model": "MODEL",
  "dataset": "gsm8k",
  "episodes": 10,
  "compression_methods": ["gptq", "lora"],
  "constraints": {
    "min_accuracy": 0.95
  }
}
```

**策略 3：分析前一次結果後調整**
```bash
# 分析結果
python scripts/run_pipeline.py analyze data/experiments/PREVIOUS_RUN

# 根據分析結果調整策略
```

### 6.4 比較不同壓縮方法

1. 執行多次實驗，每次限定不同方法：

```bash
# 只用 GPTQ
python scripts/run_pipeline.py -m gpt2 -d gsm8k -e 3 \
    -n "gpt2_gptq_only"

# 只用 AWQ
python scripts/run_pipeline.py -m gpt2 -d gsm8k -e 3 \
    -n "gpt2_awq_only"
```

2. 比較結果：

```bash
python scripts/run_pipeline.py analyze data/experiments/gpt2_gptq_only
python scripts/run_pipeline.py analyze data/experiments/gpt2_awq_only
```

---

## 7. 輸出結果解讀

### 7.1 實驗目錄結構

```
data/experiments/{experiment_name}/
├── model_spec.json           # 模型規格
├── pareto_frontier.json      # Pareto 最佳解集合
├── final_results.json        # 最終結果摘要
├── pareto_visualization.html # 互動式圖表
└── episode_001/
    ├── strategy.json         # 該 episode 使用的策略
    └── results.json          # 該 episode 的評測結果
```

### 7.2 model_spec.json 說明

```json
{
  "model_name": "gpt2",
  "model_size_gb": 0.5,
  "model_family": "gpt2",
  "parameter_count": "124M",
  "min_vram_gb": 2.0,
  "recommended_vram_gb": 4.0,
  "preferred_methods": ["autoround", "gptq", "int8"]
}
```

| 欄位 | 說明 |
|------|------|
| `model_name` | 模型名稱 |
| `model_size_gb` | 原始模型大小（GB） |
| `model_family` | 模型家族（llama、gpt2 等） |
| `parameter_count` | 參數數量 |
| `min_vram_gb` | 最低 VRAM 需求 |
| `recommended_vram_gb` | 建議 VRAM |
| `preferred_methods` | 推薦的壓縮方法 |

### 7.3 final_results.json 說明

```json
{
  "experiment_name": "gpt2_gsm8k_20260126_143022",
  "model": "gpt2",
  "dataset": "gsm8k",
  "episodes_completed": 5,
  "termination_reason": "max_episodes_reached",
  "best_solutions": {
    "accuracy": {
      "strategy": {"method": "int8"},
      "result": {"accuracy": 0.45, "compression_ratio": 1.8}
    },
    "size": {
      "strategy": {"method": "gptq", "bits": 4},
      "result": {"accuracy": 0.42, "compression_ratio": 3.9}
    }
  },
  "frontier_summary": {
    "num_solutions": 3,
    "best_accuracy": 0.45,
    "best_compression": 3.9,
    "best_latency_ms": 18.5
  }
}
```

| 欄位 | 說明 |
|------|------|
| `episodes_completed` | 完成的 episode 數 |
| `termination_reason` | 終止原因 |
| `best_solutions` | 各目標的最佳解 |
| `frontier_summary` | Pareto 前沿摘要 |

### 7.4 pareto_frontier.json 說明

```json
{
  "solutions": [
    {
      "strategy": {
        "strategy_id": "ep001_gptq_4bit",
        "methods": ["gptq"],
        "quantization_bits": 4,
        "quantization_method": "gptq"
      },
      "result": {
        "accuracy": 0.42,
        "latency_ms": 23.5,
        "memory_gb": 1.2,
        "model_size_gb": 0.13,
        "compression_ratio": 3.92,
        "co2_grams": 0.5,
        "is_pareto_optimal": true
      }
    }
  ]
}
```

**strategy 欄位**：
| 欄位 | 說明 |
|------|------|
| `strategy_id` | 策略唯一識別碼 |
| `methods` | 使用的壓縮方法列表 |
| `quantization_bits` | 量化位元數 |
| `quantization_method` | 量化方法名稱 |

**result 欄位**：
| 欄位 | 說明 |
|------|------|
| `accuracy` | 基準測試準確度（0-1） |
| `latency_ms` | 推論延遲（毫秒） |
| `memory_gb` | 執行時記憶體（GB） |
| `model_size_gb` | 壓縮後模型大小（GB） |
| `compression_ratio` | 壓縮比（原始/壓縮） |
| `co2_grams` | 碳排放（克） |

### 7.5 pareto_visualization.html 使用

用瀏覽器開啟此檔案，可以：
- 查看互動式 Pareto 圖表
- 選擇不同軸向比較指標
- 懸停查看各解的詳細資訊
- 縮放和平移圖表
- 匯出圖片

---

## 8. 故障排除

### 8.1 安裝問題

#### 問題：pip install 失敗

```
ERROR: Could not build wheels for auto-gptq
```

**解決**：
```bash
# 確認 CUDA 環境
export CUDA_HOME=/usr/local/cuda
nvcc --version

# 使用預編譯版本
pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/
```

#### 問題：import torch 失敗

```
ImportError: libcudart.so.12: cannot open shared object file
```

**解決**：
```bash
# 重新安裝對應 CUDA 版本的 PyTorch
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

### 8.2 模型下載失敗

#### 問題：Repository not found

```
OSError: meta-llama/Meta-Llama-3-8B is not a valid model repo
```

**解決**：
1. 確認模型名稱正確（到 HuggingFace 確認）
2. 如果是受限模型，需要：
   - 到模型頁面申請存取權限
   - 設定 `export HF_TOKEN=hf_...`

#### 問題：Connection timeout

```
ConnectionError: HTTPSConnectionPool(host='huggingface.co')
```

**解決**：
1. 檢查網路連線
2. 嘗試使用代理：`export HF_HUB_HTTP_PROXY=http://proxy:port`
3. 手動下載：`huggingface-cli download MODEL_NAME --local-dir ./models`

### 8.3 GPU 記憶體不足

#### 問題：CUDA out of memory

```
torch.cuda.OutOfMemoryError: CUDA out of memory
```

**解決方案（依優先順序）**：

1. **使用較小的 calibration samples**：
   系統會自動調整，或透過設定檔指定

2. **使用 QLoRA 而非 LoRA**：
   QLoRA 使用 4-bit 基礎模型，大幅減少記憶體

3. **使用較小的模型**：
   從小模型開始測試（如 gpt2、phi-2）

4. **清理 GPU 記憶體**：
   ```bash
   python -c "import torch; torch.cuda.empty_cache()"
   ```

### 8.4 執行逾時

#### 問題：任務執行過久

**解決**：
1. 設定時間預算：`--budget 2.0`（2 小時）
2. 減少 episode 數：`--episodes 5`
3. 使用 proxy 評測（只用部分資料）

### 8.5 結果異常

#### 問題：準確度為 0 或異常低

**可能原因**：
- 模型載入失敗，使用了隨機權重
- 量化過於激進
- 評測配置錯誤

**解決**：
1. 檢查日誌是否有錯誤訊息
2. 確認原始模型可正常執行
3. 嘗試較保守的壓縮設定（如 8-bit）

#### 問題：所有 episode 結果相同

**可能原因**：
- 落入 mock 模式
- 協調器決策重複

**解決**：
1. 確認量化庫已正確安裝
2. 增加 episode 數讓系統探索更多選項

---

## 9. FAQ

### 9.1 系統支援哪些模型？

理論上支援所有 HuggingFace Transformers 相容的模型，已測試包括：
- GPT-2 系列
- Llama 2/3 系列
- Mistral / Mixtral
- Phi-2/3
- Qwen 系列
- Gemma 系列

### 9.2 壓縮需要多久時間？

| 模型大小 | Episode 數 | 預估時間 |
|----------|------------|----------|
| 小（<1B） | 3-5 | 數分鐘 |
| 中（1-7B） | 5-10 | 數十分鐘至數小時 |
| 大（7-70B） | 5-10 | 數小時 |

**影響因素**：
- GPU 速度
- 評測資料集大小
- 壓縮方法複雜度

### 9.3 如何選擇最佳壓縮方法？

**一般建議**：
- **首選 GPTQ 4-bit**：平衡壓縮率與準確度
- **追求準確度用 INT8**：損失最小
- **大型模型用 QLoRA**：記憶體友善
- **追求速度用 AWQ**：推論優化好

讓系統自動探索是最好的方式，它會根據您的模型特性推薦。

### 9.4 Pareto 前沿代表什麼？

Pareto 前沿上的解代表「沒有任何其他解能同時在所有目標上勝過它」。

例如：
- 解 A：準確度 95%、壓縮 2x
- 解 B：準確度 90%、壓縮 4x

兩者都是 Pareto 最佳，因為：
- A 準確度高但壓縮低
- B 壓縮高但準確度低

選擇哪個取決於您的需求優先級。

### 9.5 如何使用壓縮後的模型？

壓縮後的模型可以直接用於推論：

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# 載入壓縮後的模型
model = AutoModelForCausalLM.from_pretrained(
    "path/to/compressed/model",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("path/to/compressed/model")

# 推論
inputs = tokenizer("Hello, how are you?", return_tensors="pt")
outputs = model.generate(**inputs)
print(tokenizer.decode(outputs[0]))
```

對於量化模型，可能需要額外的載入參數：
```python
# GPTQ 模型
from auto_gptq import AutoGPTQForCausalLM
model = AutoGPTQForCausalLM.from_quantized("path/to/gptq/model")
```

---

## 10. 附錄

### A. 支援的模型列表

| 模型家族 | 範例模型 | 建議 VRAM |
|----------|----------|-----------|
| GPT-2 | `gpt2`, `gpt2-medium`, `gpt2-large` | 2-8 GB |
| Llama 2 | `meta-llama/Llama-2-7b-hf` | 16 GB |
| Llama 3 | `meta-llama/Meta-Llama-3-8B` | 16 GB |
| Mistral | `mistralai/Mistral-7B-v0.1` | 16 GB |
| Phi | `microsoft/phi-2` | 8 GB |
| Qwen | `Qwen/Qwen1.5-7B` | 16 GB |
| Gemma | `google/gemma-7b` | 16 GB |

### B. 支援的評測基準

| 基準 | 參數名稱 | 任務類型 |
|------|----------|----------|
| GSM8K | `gsm8k` | 數學推理 |
| CommonsenseQA | `commonsenseqa` | 常識推理 |
| TruthfulQA | `truthfulqa` | 真實性 |
| HumanEval | `humaneval` | 程式碼生成 |
| BIG-Bench Hard | `bigbench_hard` | 困難推理 |
| MMLU | `mmlu` | 多領域知識 |
| HellaSwag | `hellaswag` | 情境理解 |
| ARC Easy | `arc_easy` | 科學問答 |
| ARC Challenge | `arc_challenge` | 困難科學問答 |
| WinoGrande | `winogrande` | 共指消解 |

### C. 壓縮方法比較表

| 方法 | 壓縮率 | 準確度保持 | 速度提升 | VRAM 需求 | 適用場景 |
|------|--------|------------|----------|-----------|----------|
| INT8 | 2x | 極高 | 中 | 低 | 保守壓縮 |
| GPTQ 8-bit | 2x | 高 | 中 | 低 | 平衡 |
| GPTQ 4-bit | 4x | 中高 | 高 | 低 | 推薦 |
| AWQ 4-bit | 4x | 中高 | 極高 | 低 | 追求速度 |
| AutoRound | 4x | 高 | 高 | 中 | 追求準確度 |
| LoRA | 1x | 恢復損失 | - | 中 | 微調恢復 |
| QLoRA | 4x | 恢復損失 | - | 低 | 大模型微調 |
| Pruning 30% | 1.3x | 中 | 中 | 低 | 結構優化 |

### D. 錯誤碼對照表

| 錯誤碼 | 說明 | 解決方法 |
|--------|------|----------|
| `E001` | OPENAI_API_KEY 未設定 | 設定環境變數 |
| `E002` | 模型載入失敗 | 確認模型名稱和權限 |
| `E003` | GPU 記憶體不足 | 使用較小模型或 QLoRA |
| `E004` | 評測逾時 | 增加 timeout 或減少資料量 |
| `E005` | 量化失敗 | 檢查量化庫安裝 |
| `E006` | 網路連線失敗 | 檢查網路和代理設定 |
| `E007` | 磁碟空間不足 | 清理舊實驗資料 |
| `E008` | API 配額超限 | 等待配額重置或升級方案 |

---

## 文件維護

- **作者**：Green AI Team
- **最後更新**：2026-01-26
- **版本**：1.0.0

如有問題或建議，請提交 Issue 或聯繫團隊。
