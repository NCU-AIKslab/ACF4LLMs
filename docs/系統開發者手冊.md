# 系統開發者手冊

**Agentic Compression Framework**

版本：1.0.0
最後更新：2026-01-26

---

## 目錄

- [0. 一頁總覽](#0-一頁總覽)
- [1. 快速上手](#1-快速上手)
- [2. 核心流程](#2-核心流程)
- [3. 部署與發布](#3-部署與發布)
- [4. 監控與告警](#4-監控與告警)
- [5. Runbook（值班必備）](#5-runbook值班必備)
- [6. 安全與合規](#6-安全與合規)
- [7. 附錄](#7-附錄)

---

## 0. 一頁總覽

### 0.1 系統目的

Agentic Compression Framework 是一個 **LLM 驅動的模型壓縮多代理框架**，使用 LangGraph 和 GPT-4o 自動尋找最佳壓縮策略。

**核心價值**：
- 只需輸入模型名稱與資料集，系統自動完成壓縮最佳化
- GPT-4o 作為協調器，自主決定壓縮策略
- 多目標最佳化：準確度、延遲、記憶體、模型大小、碳排放

### 0.2 系統架構圖

```
┌─────────────────────────────────────────────────────────────────────┐
│                           使用者介面                                  │
│     ┌─────────┐      ┌─────────┐      ┌─────────┐                   │
│     │   CLI   │      │   API   │      │   UI    │                   │
│     │ (Python)│      │(FastAPI)│      │ (React) │                   │
│     └────┬────┘      └────┬────┘      └────┬────┘                   │
└──────────┼────────────────┼────────────────┼────────────────────────┘
           │                │                │
           ▼                ▼                ▼
┌─────────────────────────────────────────────────────────────────────┐
│                     LangGraph 協調器核心                              │
│  ┌──────────────────────────────────────────────────────────────┐   │
│  │               Coordinator (GPT-4o)                            │   │
│  │  • 分析 Pareto 前沿                                            │   │
│  │  • 決定下一個壓縮動作                                           │   │
│  │  • 管理終止條件                                                │   │
│  └──────────────────────────────────────────────────────────────┘   │
│           │                                                          │
│  ┌────────┼────────┬────────────────┬────────────────┐              │
│  ▼        ▼        ▼                ▼                ▼              │
│ ┌────┐ ┌────┐ ┌──────┐ ┌──────────┐ ┌──────┐                       │
│ │量化│ │剪枝│ │Search│ │Evaluation│ │LoRA  │                       │
│ │Agent│ │Agent│ │Agent│ │  Agent  │ │Agent │                       │
│ └────┘ └────┘ └──────┘ └──────────┘ └──────┘                       │
│                                                                      │
│  ┌──────────────────────────────────────────────────────────────┐   │
│  │              Pareto Frontier (多目標最佳化)                     │   │
│  └──────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────┘
           │
           ▼
┌─────────────────────────────────────────────────────────────────────┐
│                          輸出層                                      │
│   data/experiments/{experiment_name}/                               │
│   ├── model_spec.json           # 模型規格                          │
│   ├── pareto_frontier.json      # Pareto 最佳解                     │
│   ├── final_results.json        # 最終結果                          │
│   └── pareto_visualization.html # 互動式圖表                        │
└─────────────────────────────────────────────────────────────────────┘
```

### 0.3 服務清單

| 服務 | 埠號 | 說明 | 檔案位置 |
|------|------|------|----------|
| CLI | - | 命令列介面 | `scripts/run_pipeline.py` |
| API Server | 8000 | FastAPI REST API | `src/api/main.py` |
| Web UI | 3000 | React 前端 | `ui/src/` |
| Prometheus | 9090 | 指標收集 | `docker-compose.yml` |
| Grafana | 3001 | 指標視覺化 | `docker-compose.yml` |

### 0.4 風險清單 Top 10

| # | 風險 | 嚴重度 | 緩解措施 |
|---|------|--------|----------|
| 1 | GPU OOM | 高 | VRAM 預估、降級到較小 batch size |
| 2 | OpenAI API 配額超限 | 高 | 監控用量、設定告警 |
| 3 | 模型下載失敗 | 中 | 重試機制、HF_TOKEN 設定 |
| 4 | 量化庫不可用 | 中 | 自動 fallback 到 mock |
| 5 | 評測超時 | 中 | timeout 設定、proxy 評測 |
| 6 | Pareto 無改善 | 中 | 收斂檢測、自動終止 |
| 7 | 磁碟空間不足 | 中 | checkpoint 清理、監控 |
| 8 | CUDA 版本不相容 | 中 | 文件說明、Docker 環境 |
| 9 | API 並發過高 | 低 | 限流、背景任務佇列 |
| 10 | 實驗結果損毀 | 低 | JSON 驗證、備份機制 |

---

## 1. 快速上手

### 1.1 環境需求

**硬體需求**：
- CPU：4 核心以上
- RAM：16GB 以上
- GPU：NVIDIA GPU（建議 8GB+ VRAM）
- 磁碟：50GB 以上可用空間

**軟體需求**：
- Python 3.10+
- CUDA Toolkit 11.8+ 和 nvcc（GPU 量化需要）
- Git

**帳號需求**：
- OpenAI API Key（必要）
- HuggingFace Token（選用，存取受限模型需要）

### 1.2 安裝步驟

```bash
# 1. 建立 conda 環境
conda create -n greenai python=3.10
conda activate greenai

# 2. 安裝基本依賴
pip install -r requirements.txt

# 3. 設定環境變數
export OPENAI_API_KEY=sk-your-api-key-here
export HF_TOKEN=hf_your_token_here  # 選用

# 4.（選用）安裝 GPU 量化庫
export CUDA_HOME=/usr/local/cuda
pip install -r requirements.quantization.txt

# 5. 驗證安裝
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
python -c "from langchain_openai import ChatOpenAI; print('OpenAI OK')"
```

### 1.3 第一次執行

```bash
# 基本執行
python scripts/run_pipeline.py \
    --model gpt2 \
    --dataset gsm8k \
    --episodes 3

# 查看推論出的模型規格
python scripts/run_pipeline.py --model gpt2 --dataset gsm8k --show-spec

# 互動模式（顯示詳細進度）
python scripts/run_pipeline.py -m gpt2 -d gsm8k -e 5 -i
```

**預期輸出**：
```
[Episode 1/3] Coordinator deciding...
[Episode 1/3] Action: quantization (gptq, 4-bit)
[Episode 1/3] Evaluating...
[Episode 1/3] ✓ Pareto improvement found!
...
Results saved to: data/experiments/gpt2_gsm8k_20260126_143022/
```

### 1.4 常見卡關

#### 問題：OPENAI_API_KEY 未設定

```
Error: OPENAI_API_KEY environment variable not set
```

**解決**：
```bash
export OPENAI_API_KEY=sk-your-api-key-here
```

#### 問題：CUDA 不可用

```
torch.cuda.is_available() returns False
```

**解決**：
1. 確認 NVIDIA 驅動已安裝：`nvidia-smi`
2. 確認 CUDA 版本相容：`nvcc --version`
3. 重新安裝 PyTorch（對應 CUDA 版本）

#### 問題：模型下載失敗（Gated Model）

```
Error: You do not have access to meta-llama/Meta-Llama-3-8B
```

**解決**：
1. 到 HuggingFace 申請模型存取權限
2. 設定 HF_TOKEN：
```bash
export HF_TOKEN=hf_your_token_here
# 或使用 huggingface-cli login
```

#### 問題：量化庫安裝失敗

```
ImportError: cannot import name 'AutoRound'
```

**解決**：系統會自動 fallback 到 mock 實作。如需真實量化：
```bash
# 確保 CUDA_HOME 正確
export CUDA_HOME=/usr/local/cuda
pip install auto-round auto-gptq autoawq --no-build-isolation
```

---

## 2. 核心流程

### 2.1 LangGraph 狀態機流程圖

```
                          ┌─────────────┐
                          │   START     │
                          └──────┬──────┘
                                 │
                                 ▼
                    ┌────────────────────────┐
                    │     COORDINATOR        │
                    │       (GPT-4o)         │
            ┌───────┤  1. 檢查終止條件        │
            │       │  2. 分析 Pareto 前沿    │
            │       │  3. 決定下一步動作      │
            │       └───────────┬────────────┘
            │                   │
            │    ┌──────────────┼──────────────┐
            │    ▼              ▼              ▼
  終止條件   │  ┌────────┐ ┌────────┐ ┌────────┐
  達成      │  │Quantize│ │Pruning │ │ Search │
            │  │  Node  │ │  Node  │ │  Node  │
            │  └───┬────┘ └───┬────┘ └───┬────┘
            │      └──────────┴──────────┘
            │                 │
            │                 ▼
            │      ┌────────────────────────┐
            │      │    EVALUATION NODE     │
            │      │  • 執行基準測試         │
            │      │  • 量測延遲/記憶體      │
            │      │  • 估算能耗/碳排        │
            │      └───────────┬────────────┘
            │                  │
            │                  ▼
            │      ┌────────────────────────┐
            │      │   UPDATE STATE NODE    │
            │      │  • 更新 Pareto 前沿     │
            │      │  • 記錄 History        │
            │      │  • 儲存 Episode 結果   │
            │      └───────────┬────────────┘
            │                  │
            │                  │ (迴圈回到 Coordinator)
            │                  │
            ▼                  ▼
      ┌─────────────────────────────────────┐
      │               END                    │
      │  • 編譯最終結果                       │
      │  • 產生視覺化圖表                     │
      │  • 儲存 Pareto 前沿                   │
      └─────────────────────────────────────┘
```

### 2.2 Coordinator 決策邏輯

Coordinator 使用 GPT-4o 進行決策，輸入包含：

1. **Pareto 前沿摘要**：目前最佳解的分布
2. **最近 5 次 History**：之前嘗試的策略與結果
3. **Skill 推薦**：根據模型特性推薦的方法
4. **模型規格**：VRAM 需求、支援的方法

**決策輸出格式**：
```json
{
  "action": "quantization|pruning|lora|qlora|asvd|pipeline|search|end",
  "method": "gptq|autoround|awq|int8",
  "bits": 4,
  "reasoning": "選擇 GPTQ 4-bit 因為..."
}
```

**關鍵程式碼位置**：`src/coordinator/langgraph_coordinator.py:_coordinator_node()`

### 2.3 Agent 節點說明

#### Quantization Agent（`src/agents/quantization_agent.py`）

| Tool | 功能 | 主要參數 |
|------|------|----------|
| `quantize_model` | 執行量化 | method, bit_width, calibration_samples |
| `apply_lora_finetuning` | LoRA 微調 | rank, alpha, target_modules |
| `apply_qlora_finetuning` | QLoRA 微調 | 結合 4-bit 量化與 LoRA |
| `apply_asvd_compression` | ASVD 壓縮 | rank_ratio |
| `estimate_quantization_vram` | VRAM 預估 | - |

**支援的量化方法**：
- **AutoRound**：Intel 開發，4/8-bit
- **GPTQ**：最常用，4/8-bit，需 calibration
- **AWQ**：Activation-aware，4-bit
- **INT8**：8-bit 動態量化

#### Evaluation Agent（`src/agents/evaluation_agent.py`）

| Tool | 功能 |
|------|------|
| `evaluate_model` | 完整基準測試 |
| `run_proxy_evaluation` | 快速評測（10% 樣本） |
| `measure_inference_latency` | 延遲量測 |
| `measure_memory_usage` | 記憶體量測 |
| `estimate_energy_consumption` | 能耗估算 |

**支援的基準測試**：
- GSM8K, CommonsenseQA, TruthfulQA, HumanEval, BIG-Bench Hard
- MMLU, HellaSwag, ARC, WinoGrande

#### Search Agent（`src/agents/search_agent.py`）

| Tool | 功能 |
|------|------|
| `bayesian_optimization_search` | 貝葉斯最佳化搜尋 |
| `evolutionary_search` | 演化演算法搜尋 |
| `multi_armed_bandit_search` | 多臂老虎機選擇 |
| `check_early_stopping` | 提前停止檢查 |

#### Pruning Agent（`src/agents/pruning_agent.py`）

| Tool | 功能 |
|------|------|
| `prune_model` | 模型剪枝 |
| `estimate_pruning_speedup` | 加速預估 |

### 2.4 失敗與降級策略

系統設計為優雅降級，當元件不可用時自動 fallback：

| 元件 | 真實實作 | Fallback |
|------|----------|----------|
| 量化（GPTQ/AWQ/AutoRound） | 真實壓縮 | Mock 回傳模擬結果 |
| lm-eval 評測 | EleutherAI harness | 內建 BenchmarkRunner |
| 能耗追蹤 | codecarbon | 估算公式 |
| VRAM 預估 | HF Hub API | 靜態資料庫 |

**Fallback 偵測邏輯**（`src/tools/quantization_wrapper.py`）：
```python
try:
    from auto_gptq import AutoGPTQForCausalLM
    GPTQ_AVAILABLE = True
except ImportError:
    GPTQ_AVAILABLE = False
    logger.warning("GPTQ not available, falling back to mock")
```

---

## 3. 部署與發布

### 3.1 環境差異

| 設定 | Development | Production |
|------|-------------|------------|
| DEBUG | True | False |
| CORS Origins | localhost:3000,8000 | 指定域名清單 |
| Log Level | DEBUG | INFO |
| Mock Mode | 可選 | 禁用 |

**環境變數設定**：

```bash
# Development
export ENVIRONMENT=development
export ALLOWED_ORIGINS=""  # 使用預設 localhost

# Production
export ENVIRONMENT=production
export ALLOWED_ORIGINS=https://myapp.com,https://admin.myapp.com
export CORS_ALLOW_CREDENTIALS=true
export ALLOWED_ORIGIN_REGEX=https://.*\.pages\.dev
```

### 3.2 Docker Compose 部署

**啟動服務**：
```bash
# 建置並啟動所有服務
docker-compose up -d

# 僅啟動 API 和 Worker
docker-compose up -d api worker

# 查看日誌
docker-compose logs -f api
```

**服務說明**：
```yaml
services:
  redis:        # 任務佇列（埠 6379）
  api:          # FastAPI 服務（埠 8000）
  worker:       # 背景任務處理（2 replicas）
  dashboard:    # Streamlit UI（埠 8501）
  prometheus:   # 指標收集（埠 9090）
  grafana:      # 視覺化（埠 3000）
```

**GPU 配置**：
```yaml
services:
  api:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

### 3.3 CI/CD 與發版流程

**建議的分支策略**：
- `main`：穩定版本
- `develop`：開發整合
- `feature/*`：功能開發
- `hotfix/*`：緊急修復

**發版流程**：
1. 從 `develop` 建立 release branch
2. 執行完整測試：`pytest tests/ -v`
3. 更新版本號（`src/api/main.py`）
4. 合併到 `main` 並打 tag
5. 部署到 production

### 3.4 API CORS 設定

**設定位置**：`src/api/main.py:60-93`

```python
# 正確設定範例
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://myapp.com", "https://admin.myapp.com"],
    allow_origin_regex=r"https://.*\.pages\.dev",
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["Content-Type", "Authorization", "X-Request-ID"],
)
```

**注意事項**：
- 不要在 production 使用 `allow_origins=["*"]` 搭配 `allow_credentials=True`
- 使用 `allow_origin_regex` 處理動態域名（如 Cloudflare Pages preview）

---

## 4. 監控與告警

### 4.1 Prometheus + Grafana 儀表板

**Prometheus 設定**（`prometheus.yml`）：
```yaml
scrape_configs:
  - job_name: 'api'
    static_configs:
      - targets: ['api:8000']
    metrics_path: /metrics
```

**Grafana 存取**：
- URL：`http://localhost:3000`
- 預設帳號：admin / admin

**建議監控指標**：
- API 請求延遲（P50, P95, P99）
- 任務佇列長度
- GPU 記憶體使用率
- 任務完成率 / 失敗率
- OpenAI API 呼叫次數

### 4.2 GPU 資源監控

**API 端點**：`GET /gpu`

**回傳範例**：
```json
{
  "available": true,
  "gpus": [
    {
      "index": 0,
      "name": "NVIDIA GeForce RTX 4090",
      "memory_total_gb": 24.0,
      "memory_used_gb": 8.5,
      "memory_utilization": 35.4,
      "compute_capability": "8.9"
    }
  ]
}
```

**Resource Monitor Agent**（`src/agents/resource_monitor_agent.py`）：
```python
# 程式內查詢 GPU 狀態
from src.agents.resource_monitor_agent import check_gpu_availability
status = check_gpu_availability()
```

### 4.3 日誌系統與 Request ID 追蹤

**日誌位置**：
- 開發環境：stdout
- Docker 環境：`./logs/` volume
- API 任務日誌：透過 `GET /jobs/{job_id}/logs` 取得

**Request ID 追蹤**：
API 支援 `X-Request-ID` header，可用於追蹤跨服務請求。

**日誌格式**：
```
[HH:MM:SS] [INFO] Message content
[14:30:22] [Episode 3/10] Coordinator deciding...
```

---

## 5. Runbook（值班必備）

### 5.1 API 逾時 / 5xx 錯誤

**症狀**：API 回傳 500 或 504 錯誤

**診斷步驟**：
```bash
# 1. 檢查服務狀態
docker-compose ps

# 2. 查看 API 日誌
docker-compose logs --tail=100 api

# 3. 檢查健康狀態
curl http://localhost:8000/health
```

**常見原因與解決**：
| 原因 | 解決 |
|------|------|
| OpenAI API 逾時 | 增加 timeout 設定，或重試 |
| GPU 記憶體不足 | 重啟服務、清理記憶體 |
| 服務崩潰 | `docker-compose restart api` |

### 5.2 GPU OOM 處理

**症狀**：`CUDA out of memory` 錯誤

**立即處理**：
```bash
# 1. 檢查 GPU 記憶體
nvidia-smi

# 2. 清理 PyTorch cache
python -c "import torch; torch.cuda.empty_cache()"

# 3. 重啟服務（最後手段）
docker-compose restart api worker
```

**預防措施**：
- 使用 `estimate_quantization_vram` 工具預估記憶體
- 降低 batch_size
- 使用較小的 calibration_samples
- 考慮使用 QLoRA 而非 LoRA

### 5.3 模型下載失敗

**症狀**：`OSError: Can't load tokenizer` 或 `Repository not found`

**診斷**：
```bash
# 檢查 HF Token
python -c "from huggingface_hub import HfApi; print(HfApi().whoami())"
```

**解決**：
1. 確認 HF_TOKEN 設定正確
2. 確認已申請模型存取權限
3. 檢查網路連線
4. 嘗試手動下載：`huggingface-cli download MODEL_NAME`

### 5.4 OpenAI API 配額超限

**症狀**：`RateLimitError: Rate limit exceeded`

**立即處理**：
1. 暫停正在執行的任務
2. 等待配額重置（通常 1 分鐘）
3. 或切換到備用 API Key

**預防措施**：
- 設定 OpenAI 用量告警
- 實作指數退避重試
- 考慮使用較便宜的模型（如用於非關鍵決策）

### 5.5 Pareto 前沿無改善

**症狀**：連續多個 episode 沒有 Pareto 改善

**說明**：這是正常的收斂行為，系統設定為連續 5 次無改善後自動終止。

**檢查**：
```bash
# 查看 Pareto 前沿
cat data/experiments/EXPERIMENT_DIR/pareto_frontier.json | jq '.solutions | length'

# 查看終止原因
cat data/experiments/EXPERIMENT_DIR/final_results.json | jq '.termination_reason'
```

**調整選項**：
- 增加 `--episodes` 數量
- 嘗試不同的初始策略
- 檢查是否已達到壓縮極限

---

## 6. 安全與合規

### 6.1 API Key 管理

**必要的環境變數**：
| 變數 | 用途 | 敏感度 |
|------|------|--------|
| `OPENAI_API_KEY` | GPT-4o 協調器 | 高 |
| `HF_TOKEN` | HuggingFace 模型存取 | 中 |

**最佳實務**：
- 永遠不要將 API Key 寫入程式碼或 commit
- 使用環境變數或 secrets manager
- 定期輪換 API Key
- 設定 API Key 用量限制

**錯誤處理**：
```bash
# 不要這樣做
export OPENAI_API_KEY=sk-xxx >> ~/.bashrc  # 會留下歷史記錄

# 建議做法
# 使用 .env 檔案（已加入 .gitignore）
cp .env.example .env
# 編輯 .env 填入 Key
```

### 6.2 模型權限（Gated Models）

某些模型（如 Llama、Gemma）需要先申請存取權限：

1. 到 HuggingFace 模型頁面
2. 點擊「Request access」
3. 同意使用條款
4. 等待核准（通常即時）
5. 設定 HF_TOKEN

**確認權限**：
```python
from huggingface_hub import model_info
info = model_info("meta-llama/Meta-Llama-3-8B")
print(f"Gated: {info.gated}")
```

### 6.3 資料隔離與清理

**實驗資料位置**：
- `data/experiments/`：實驗結果（JSON、HTML）
- `checkpoints/`：模型 checkpoint（可能很大）
- `mlruns/`：MLflow 追蹤資料

**清理指令**：
```bash
# 清理舊實驗（保留最近 7 天）
find data/experiments -type d -mtime +7 -exec rm -rf {} +

# 清理暫存 checkpoint
rm -rf checkpoints/tmp_*

# 清理 PyTorch cache
rm -rf ~/.cache/torch/
```

**資料備份建議**：
- 定期備份 `data/experiments/` 到遠端儲存
- 重要的 Pareto 前沿結果應歸檔保存

---

## 7. 附錄

### A. 重要連結

| 資源 | 連結 |
|------|------|
| 原始碼倉庫 | `/mnt/ssd_1TB/Green_AI` |
| API 文件 | `http://localhost:8000/docs` |
| Grafana Dashboard | `http://localhost:3000` |
| HuggingFace | `https://huggingface.co` |
| OpenAI Platform | `https://platform.openai.com` |

### B. 常用指令

```bash
# === 環境驗證 ===
# 檢查 GPU
nvidia-smi
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"

# 檢查 OpenAI
python -c "from langchain_openai import ChatOpenAI; ChatOpenAI().invoke('test')"

# === 執行 ===
# 基本壓縮
python scripts/run_pipeline.py -m MODEL -d DATASET -e EPISODES

# 互動模式
python scripts/run_pipeline.py -m MODEL -d DATASET -e 10 -i

# 顯示模型規格
python scripts/run_pipeline.py -m MODEL -d DATASET --show-spec

# 分析實驗結果
python scripts/run_pipeline.py analyze data/experiments/EXPERIMENT_DIR

# === 測試 ===
# 執行所有測試
pytest tests/ -v

# 執行特定測試
pytest tests/test_basic.py::test_spec_inference -v

# === API ===
# 啟動 API 服務
uvicorn src.api.main:app --reload --port 8000

# 健康檢查
curl http://localhost:8000/health

# === Docker ===
# 啟動所有服務
docker-compose up -d

# 查看日誌
docker-compose logs -f api

# 重啟服務
docker-compose restart api
```

### C. 交接清單

新接手的開發者應完成以下事項：

- [ ] 取得 OpenAI API Key 存取權限
- [ ] 取得 HuggingFace Token（如需存取受限模型）
- [ ] 設定本地開發環境（參照 1.2）
- [ ] 成功執行測試：`pytest tests/ -v`
- [ ] 成功執行範例：`python scripts/run_pipeline.py -m gpt2 -d gsm8k -e 3`
- [ ] 閱讀 `docs/ARCHITECTURE.md` 了解系統架構
- [ ] 取得生產環境存取權限（如適用）
- [ ] 加入相關 Slack/Teams 頻道（如適用）
- [ ] 了解值班輪替與通報流程（如適用）

---

## 文件維護

- **作者**：Green AI Team
- **最後更新**：2026-01-26
- **版本**：1.0.0

如有問題或建議，請提交 Issue 或聯繫團隊。
