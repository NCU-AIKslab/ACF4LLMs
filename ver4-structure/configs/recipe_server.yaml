# Server throughput recipe: Optimize for high-throughput serving with vLLM
name: "server_throughput"
description: "Optimize for maximum throughput in serving scenarios using vLLM PagedAttention"
version: "1.0"

# Base model configuration
base_model: "Qwen/Qwen2.5-Math-7B"
tokenizer_model: "Qwen/Qwen2.5-Math-7B"

# Multi-objective targets (weights for optimization)
targets:
  objective_weights:
    accuracy: 0.7      # Balanced with throughput
    throughput: 1.0    # Primary objective (tokens/sec)
    latency: 0.8       # Important for user experience
    vram: 0.9          # Memory efficiency important for serving
    co2: 0.3           # Environmental consideration

# Optimization pipeline stages
pipeline:
  - stage: quantize_awq
    enabled: true
    args:
      group_size: 128
      zero_point: true
      version: "GEMM"
      calib_data_size: 512

  - stage: kv_runtime
    enabled: true
    args:
      flash_attn: false  # vLLM handles attention optimization
      vllm: true
      max_model_len: 2048
      max_num_seqs: 256
      tensor_parallel_size: 1
      gpu_memory_utilization: 0.9
      kv_cache_dtype: "fp8"
      quantization: "awq"
      enforce_eager: false
      enable_chunked_prefill: true
      max_num_batched_tokens: 4096

  - stage: rag
    enabled: false  # Disable for serving simplicity

# Generation/decoding parameters (optimized for throughput)
decode:
  temperature: 0.0    # Deterministic for consistency
  top_p: 1.0
  top_k: -1
  max_tokens: 256     # Note: vLLM uses 'max_tokens' not 'max_new_tokens'
  best_of: 1
  use_beam_search: false
  n: 1                # Number of completions
  frequency_penalty: 0.0
  presence_penalty: 0.0
  repetition_penalty: 1.0
  length_penalty: 1.0
  stop_token_ids: []
  include_stop_str_in_output: false
  ignore_eos: false

# Dataset configuration
dataset:
  name: "openai/gsm8k"
  val_split_size: 500
  augmentation_recipes: []  # No augmentation for serving focus
  max_samples_train: null
  max_samples_eval: 1000  # Limited for faster iteration
  random_seed: 42

# Evaluation configuration (throughput-focused)
evaluation:
  batch_size: 64      # Large batches for throughput measurement
  num_warmup_batches: 5
  save_predictions: false
  error_analysis: false
  carbon_tracking: true
  throughput_benchmark: true
  concurrent_requests: [1, 4, 8, 16, 32, 64]  # Test various loads

# Resource limits
resource_limits:
  max_vram_gb: 24
  max_training_hours: 1   # Minimal training
  max_eval_time_minutes: 30
  early_stopping_patience: 2

# vLLM server configuration
vllm_server:
  host: "0.0.0.0"
  port: 8000
  api_key: null
  served_model_name: "qwen-math-optimized"
  chat_template: null
  response_role: "assistant"
  ssl_keyfile: null
  ssl_certfile: null
  ssl_ca_certs: null
  ssl_cert_reqs: 0
  root_path: null
  middleware: []
  return_tokens_as_token_ids: false

  # Engine configuration
  engine:
    model: null  # Will be set from base_model
    tokenizer: null
    tokenizer_mode: "auto"
    trust_remote_code: true
    download_dir: null
    load_format: "auto"
    dtype: "bfloat16"
    kv_cache_dtype: "fp8"
    quantization_param_path: null
    max_model_len: 2048
    guided_decoding_backend: "outlines"
    distributed_executor_backend: null
    worker_use_ray: false
    pipeline_parallel_size: 1
    tensor_parallel_size: 1
    max_parallel_loading_workers: null
    ray_workers_use_nsight: false
    block_size: 16
    enable_prefix_caching: false
    disable_sliding_window: false
    use_v2_block_manager: true
    swap_space: 4
    cpu_offload_gb: 0
    gpu_memory_utilization: 0.9
    max_num_batched_tokens: 4096
    max_num_seqs: 256
    max_logprobs: 20
    disable_log_stats: false
    quantization: "awq"
    rope_scaling: null
    rope_theta: null
    enforce_eager: false
    max_context_len_to_capture: 8192
    max_seq_len_to_capture: 8192
    disable_custom_all_reduce: false
    tokenizer_pool_size: 0
    tokenizer_pool_type: "ray"
    tokenizer_pool_extra_config: {}
    enable_lora: false
    max_loras: 1
    max_lora_rank: 16
    enable_prompt_adapter: false
    max_prompt_adapters: 1
    max_prompt_adapter_token: 0
    fully_sharded_loras: false
    lora_extra_vocab_size: 256
    lora_dtype: "auto"
    long_lora_scaling_factors: null
    max_cpu_loras: null
    device: "auto"
    num_scheduler_steps: 1
    multi_step_stream_outputs: false
    enable_chunked_prefill: true
    speculative_model: null
    num_speculative_tokens: null
    speculative_draft_tensor_parallel_size: null
    speculative_max_model_len: null
    speculative_disable_by_batch_size: null
    ngram_prompt_lookup_max: null
    ngram_prompt_lookup_min: null
    spec_decoding_acceptance_method: "rejection_sampler"
    typical_acceptance_sampler_posterior_threshold: null
    typical_acceptance_sampler_posterior_alpha: null
    disable_logprobs_during_spec_decoding: null
    model_loader_extra_config: {}
    preemption_mode: null
    served_model_name: ["qwen-math-optimized"]
    qlora_adapter_name_or_path: null
    otlp_traces_endpoint: null

# Logging and tracking
tracking:
  mlflow_experiment: "gsm8k_server_optimization"
  mlflow_tracking_uri: "file:./mlruns"
  log_level: "INFO"
  save_checkpoints: false
  checkpoint_dir: "./checkpoints/server_throughput"

# Hardware optimizations
hardware:
  use_mixed_precision: true
  dataloader_pin_memory: true
  torch_compile: false  # vLLM handles optimizations
  gradient_checkpointing: false

# Benchmarking configuration
benchmark:
  enabled: true
  concurrent_requests: [1, 2, 4, 8, 16, 32, 64, 128]
  request_rate: [1, 5, 10, 20, 50, 100]  # requests per second
  input_lengths: [50, 100, 200, 500]     # input token lengths
  output_lengths: [50, 100, 200, 300]    # output token lengths
  duration_seconds: 60                    # benchmark duration per configuration
  warmup_seconds: 10