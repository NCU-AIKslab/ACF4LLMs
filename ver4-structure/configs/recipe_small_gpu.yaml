# Small GPU recipe: Optimize for resource-constrained environments
name: "small_gpu"
description: "Maximize efficiency on GPUs with limited memory (8-16GB) using aggressive compression"
version: "1.0"

# Base model configuration
base_model: "Qwen/Qwen2.5-Math-7B"
tokenizer_model: "Qwen/Qwen2.5-Math-7B"

# Multi-objective targets (weights for optimization)
targets:
  objective_weights:
    accuracy: 0.8      # Still important
    latency: 0.6       # Moderate importance
    vram: 1.0          # Critical constraint
    co2: 0.4           # Efficiency matters

# Optimization pipeline stages
pipeline:
  - stage: quantize_gptq
    enabled: true
    args:
      bits: 4
      group_size: 32
      desc_act: true
      damp_percent: 0.1
      calib_data_size: 128  # Minimal calibration for speed
      use_exllama: false    # Better memory efficiency

  - stage: prune_sparsity
    enabled: true
    args:
      sparsity_ratio: 0.3
      structured_sparsity: "2:4"
      layers_to_prune: ["attention.ffn"]  # Conservative pruning
      recovery_finetune: false            # Skip to save memory

  - stage: distill_lora
    enabled: true
    args:
      lora_r: 8         # Very small rank
      lora_alpha: 16
      lora_dropout: 0.05
      target_modules: ["q_proj", "v_proj"]  # Minimal modules
      learning_rate: 1e-4
      num_epochs: 1
      warmup_ratio: 0.03
      weight_decay: 0.0
      gradient_accumulation_steps: 8  # Simulate larger batches
      per_device_train_batch_size: 1  # Single sample per batch
      dataloader_num_workers: 2
      save_strategy: "no"
      evaluation_strategy: "no"
      logging_steps: 100
      gradient_checkpointing: true    # Trade compute for memory
      dataloader_pin_memory: false    # Save system memory

  - stage: kv_runtime
    enabled: true
    args:
      flash_attn: true
      vllm: false
      max_sequence_length: 512  # Short contexts only
      kv_cache_dtype: "fp8"     # Aggressive KV compression

  - stage: rag
    enabled: false  # Too memory intensive

# Generation/decoding parameters (memory efficient)
decode:
  temperature: 0.1    # Low temperature for consistent, short outputs
  top_p: 0.85
  top_k: 20
  max_new_tokens: 128  # Short responses
  do_sample: true
  num_beams: 1
  repetition_penalty: 1.1
  length_penalty: 0.8  # Encourage brevity

# Dataset configuration
dataset:
  name: "openai/gsm8k"
  val_split_size: 200   # Very small validation
  augmentation_recipes: []
  max_samples_train: 2000  # Limited training data
  max_samples_eval: 500
  random_seed: 42

# Evaluation configuration
evaluation:
  batch_size: 1       # Single sample at a time
  num_warmup_batches: 1
  save_predictions: false
  error_analysis: false
  carbon_tracking: false  # Skip to save overhead

# Resource limits (strict)
resource_limits:
  max_vram_gb: 12     # Strict VRAM limit
  max_training_hours: 1
  max_eval_time_minutes: 10
  early_stopping_patience: 2
  max_checkpoint_size_gb: 1

# Memory optimization settings
memory_optimization:
  cpu_offload: true
  offload_optimizer: true
  offload_param: true
  zero_stage: 2
  activation_checkpointing: true
  gradient_compression: true
  parameter_tying: true
  use_8bit_optimizer: true
  empty_cache_freq: 10  # Clear cache every 10 steps

# Logging and tracking (minimal)
tracking:
  mlflow_experiment: "gsm8k_small_gpu_optimization"
  mlflow_tracking_uri: "file:./mlruns"
  log_level: "ERROR"    # Minimal logging
  save_checkpoints: false
  checkpoint_dir: null

# Hardware optimizations
hardware:
  use_mixed_precision: true
  dataloader_pin_memory: false   # Save system RAM
  torch_compile: false          # May increase memory usage
  gradient_checkpointing: true  # Essential for memory
  cpu_threads: 2                # Limit CPU usage

# Model loading optimizations
model_loading:
  low_cpu_mem_usage: true
  torch_dtype: "bfloat16"
  device_map: "auto"
  max_memory: {0: "10GB"}       # Reserve some VRAM
  offload_folder: "./offload"
  offload_state_dict: true