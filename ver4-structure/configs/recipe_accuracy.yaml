# Accuracy-first recipe: Maximize GSM8K accuracy with minimal resource constraints
name: "accuracy_first"
description: "Optimize for maximum accuracy on GSM8K with QLoRA fine-tuning"
version: "1.0"

# Base model configuration
base_model: "Qwen/Qwen2.5-Math-7B"
tokenizer_model: "Qwen/Qwen2.5-Math-7B"

# Multi-objective targets (weights for optimization)
targets:
  objective_weights:
    accuracy: 1.0      # Primary objective
    latency: 0.1       # Secondary consideration
    vram: 0.1          # Secondary consideration
    co2: 0.05          # Minimal consideration

# Optimization pipeline stages
pipeline:
  - stage: quantize_bnb
    enabled: true
    args:
      quant_type: "nf4"
      compute_dtype: "bfloat16"
      bnb_4bit_use_double_quant: true
      bnb_4bit_quant_type: "nf4"

  - stage: distill_lora
    enabled: true
    args:
      lora_r: 32
      lora_alpha: 64
      lora_dropout: 0.05
      target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      learning_rate: 2.5e-4
      num_epochs: 2
      warmup_ratio: 0.1
      weight_decay: 0.01
      gradient_accumulation_steps: 4
      per_device_train_batch_size: 1
      dataloader_num_workers: 4
      save_strategy: "epoch"
      evaluation_strategy: "steps"
      eval_steps: 100
      logging_steps: 10
      save_total_limit: 2

  - stage: kv_runtime
    enabled: true
    args:
      flash_attn: true
      vllm: false
      max_sequence_length: 2048

  - stage: rag
    enabled: true
    args:
      top_k: 4
      retrieval_threshold: 0.7
      kb_path: "./kb/math_facts.faiss"

# Generation/decoding parameters
decode:
  temperature: 0.2
  top_p: 0.95
  top_k: 50
  max_new_tokens: 256
  do_sample: true
  num_beams: 1
  repetition_penalty: 1.0
  length_penalty: 1.0

# Dataset configuration
dataset:
  name: "openai/gsm8k"
  val_split_size: 1000
  augmentation_recipes:
    - "calculator_annotation"
    - "numeric_jitter"
  max_samples_train: null  # Use all training data
  max_samples_eval: null   # Use all evaluation data
  random_seed: 42

# Evaluation configuration
evaluation:
  batch_size: 8
  num_warmup_batches: 2
  save_predictions: true
  error_analysis: true
  carbon_tracking: true

# Resource limits
resource_limits:
  max_vram_gb: 24
  max_training_hours: 8
  max_eval_time_minutes: 60
  early_stopping_patience: 8

# Logging and tracking
tracking:
  mlflow_experiment: "gsm8k_accuracy_optimization"
  mlflow_tracking_uri: "file:./mlruns"
  log_level: "INFO"
  save_checkpoints: true
  checkpoint_dir: "./checkpoints/accuracy_first"

# Hardware optimizations
hardware:
  use_mixed_precision: true
  dataloader_pin_memory: true
  torch_compile: false  # May have compatibility issues
  gradient_checkpointing: true