# LLM-driven agents configuration
# Configuration for LangChain and LangGraph powered agents

# LLM Configuration for agents
llm:
  provider: "openai"           # openai, anthropic, google
  model: "gpt-4"              # gpt-4, gpt-3.5-turbo, claude-3-sonnet, gemini-pro
  temperature: 0.1            # Low temperature for consistent decisions
  max_tokens: 2000           # Token limit for responses
  timeout: 60                # Timeout in seconds

# Model configuration
model:
  base_model: "google/gemma-3-4b-it"
  sequence_length: 4096
  max_model_len: 4096

# Hardware configuration
hardware:
  gpu: "NVIDIA RTX 4090"
  gpu_memory_utilization: 0.85
  vram_limit_gb: 22
  tensor_parallel_size: 1

# LLM Agent configurations
agents:
  quantization:
    enabled: true
    llm:
      provider: "openai"
      model: "gpt-4"
      temperature: 0.1
    decision_criteria:
      accuracy_priority: 0.8
      speed_priority: 0.6
      memory_priority: 0.7
    available_methods:
      - "awq"
      - "gptq"
      - "bitsandbytes"
    default_parameters:
      bits: 4
      group_size: 128

  pruning_sparsity:
    enabled: true
    llm:
      provider: "openai"
      model: "gpt-4"
      temperature: 0.1
    decision_criteria:
      performance_priority: 0.7
      compression_priority: 0.8
      hardware_efficiency: 0.6
    available_methods:
      - "unstructured"
      - "structured"
      - "nm_sparsity"
      - "attention_head"
      - "gradual"
    default_parameters:
      sparsity_ratio: 0.5
      importance_metric: "magnitude"

  distillation:
    enabled: true
    llm:
      provider: "openai"
      model: "gpt-3.5-turbo"  # Can use cheaper model for simpler tasks
      temperature: 0.2
    decision_criteria:
      knowledge_preservation: 0.9
      efficiency_gain: 0.6
      training_cost: 0.5
    teacher_models:
      - "google/gemma-3-4b-it"
      - "meta-llama/Llama-2-7b-hf"
    student_sizes:
      - "1b"
      - "2b"
      - "270m"

  kv_longcontext:
    enabled: true
    llm:
      provider: "openai"
      model: "gpt-4"
      temperature: 0.1
    optimization_methods:
      - "flash_attention"
      - "paged_attention"
      - "sliding_window"
      - "kv_compression"
    context_lengths:
      - 2048
      - 4096
      - 8192
      - 16384

  perf_carbon:
    enabled: true
    llm:
      provider: "anthropic"
      model: "claude-3-haiku"  # Fast model for monitoring tasks
      temperature: 0.05
    monitoring_metrics:
      - "latency"
      - "throughput"
      - "energy_consumption"
      - "co2_emissions"
      - "gpu_utilization"
    reporting_frequency: "real_time"

# Workflow configuration for LangGraph
workflow:
  max_iterations: 10
  error_threshold: 3
  timeout_minutes: 60
  parallel_execution: false

  # Agent execution order strategies
  execution_strategy: "intelligent"  # intelligent, sequential, parallel

  # Decision-making criteria for routing
  routing_criteria:
    performance_weight: 0.4
    accuracy_weight: 0.3
    efficiency_weight: 0.3

  # LLM-driven coordination
  coordinator_llm:
    provider: "openai"
    model: "gpt-4"
    temperature: 0.2

# Evaluation configuration
evaluation:
  # Use subset for faster LLM-driven evaluation
  gsm8k:
    enabled: true
    num_samples: 100  # Reduced for faster iteration

  truthfulqa:
    enabled: true
    num_samples: 50

  commonsenseqa:
    enabled: true
    num_samples: 100

  humaneval:
    enabled: true
    num_samples: 50  # Reduced set

  bigbench_hard:
    enabled: true
    num_samples: 50

# Constraints for LLM decision making
constraints:
  max_accuracy_drop: 0.05     # 5% max accuracy drop
  p95_latency_ms: 200         # 200ms max latency
  max_vram_gb: 22            # VRAM constraint
  energy_budget_kwh: 5.0     # Energy budget per optimization
  co2_budget_kg: 2.0         # CO2 budget

# Objective weights for multi-objective optimization
objective_weights:
  accuracy: 1.0              # Maximize accuracy
  latency: -1.0             # Minimize latency
  vram: -1.0                # Minimize VRAM usage
  energy: -1.0              # Minimize energy consumption
  co2e: -1.0                # Minimize CO2 emissions

# LLM Provider API Keys (set as environment variables)
# OPENAI_API_KEY=your_openai_api_key
# ANTHROPIC_API_KEY=your_anthropic_api_key
# GOOGLE_API_KEY=your_google_api_key

# Advanced LLM settings
llm_advanced:
  retry_attempts: 3
  backoff_factor: 2
  cache_responses: true
  cache_ttl_hours: 24

  # Token usage optimization
  max_context_tokens: 8000
  response_token_limit: 1000

  # Safety settings
  content_filter: true
  response_validation: true

  # Logging
  log_requests: true
  log_responses: false  # Set to true for debugging