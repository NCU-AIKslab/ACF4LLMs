"""
ä½¿ç”¨ Deep Agent å·¥å…· - ç„¡éœ€ API Key

å±•ç¤ºå¦‚ä½•åœ¨æ²’æœ‰ API key çš„æƒ…æ³ä¸‹ä½¿ç”¨æ‰€æœ‰å£“ç¸®å·¥å…·ã€‚
"""

import json
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def example_1_lora_estimation():
    """ç¤ºä¾‹ 1: ä¼°ç®— LoRA å£“ç¸®æ•ˆæœ"""
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 1: ä¼°ç®— LoRA å£“ç¸®æ•ˆæœï¼ˆç„¡éœ€ API keyï¼‰")
    print("=" * 80)

    from src.agentic_compression.agents.sub_agents.lora_sub_agent import (
        EstimateLoRAImpactTool,
        ConfigureLoRATool,
    )

    # ä¼°ç®—ä¸åŒ rank çš„ LoRA å½±éŸ¿
    tool = EstimateLoRAImpactTool()

    for rank in [4, 8, 16]:
        print(f"\nğŸ” æ¸¬è©¦ LoRA rank={rank}:")
        result = tool._run(
            base_model="meta-llama/Llama-2-7b-hf",
            rank=rank,
        )
        result_dict = json.loads(result)
        print(f"  å£“ç¸®æ¯”: {result_dict['compression_ratio']}x")
        print(f"  å¯è¨“ç·´åƒæ•¸: {result_dict['trainable_params_percent']}%")
        print(f"  æ¨è–¦ç”¨é€”: {result_dict['recommended_use_cases'][0]}")

    # é…ç½® LoRA
    print(f"\nâš™ï¸  é…ç½® LoRA (rank=8):")
    config_tool = ConfigureLoRATool()
    result = config_tool._run(
        base_model="meta-llama/Llama-2-7b-hf",
        rank=8,
        alpha=16,
    )
    config = json.loads(result)
    print(f"  ç›®æ¨™æ¨¡å¡Š: {config['config']['target_modules']}")
    print(f"  é©é…å™¨å¤§å°: {config['estimated_adapter_size_m']} M åƒæ•¸")


def example_2_distillation_planning():
    """ç¤ºä¾‹ 2: è¦åŠƒçŸ¥è­˜è’¸é¤¾"""
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 2: è¦åŠƒçŸ¥è­˜è’¸é¤¾ï¼ˆç„¡éœ€ API keyï¼‰")
    print("=" * 80)

    from src.agentic_compression.agents.sub_agents.distillation_sub_agent import (
        SetupDistillationTool,
        EstimateDistillationTool,
    )

    # è¨­ç½®è’¸é¤¾
    setup_tool = SetupDistillationTool()
    result = setup_tool._run(
        teacher_model="meta-llama/Llama-2-7b-hf",
        student_scale=0.5,  # å­¸ç”Ÿæ¨¡å‹æ˜¯è€å¸«çš„ 50%
        temperature=2.0,
        alpha=0.7,
    )
    config = json.loads(result)

    print(f"\nğŸ“š è’¸é¤¾é…ç½®:")
    print(f"  è€å¸«: {config['teacher']['model_name']} ({config['teacher']['size_m']} M)")
    print(f"  å­¸ç”Ÿ: {config['student']['size_m']} M")
    print(f"  å£“ç¸®æ¯”: {config['compression_ratio']}x")
    print(f"  é æœŸç²¾åº¦æå¤±: {config['expected_accuracy_loss']}")
    print(f"  é æœŸåŠ é€Ÿ: {config['estimated_speedup']}")

    # ä¼°ç®—ä¸åŒå£“ç¸®æ¯”
    print(f"\nğŸ”¬ æ¯”è¼ƒä¸åŒå£“ç¸®æ¯”:")
    estimate_tool = EstimateDistillationTool()

    for ratio in [2.0, 3.0, 5.0]:
        result = estimate_tool._run(
            teacher_model="meta-llama/Llama-2-7b-hf",
            compression_ratio=ratio,
        )
        est = json.loads(result)
        metrics = est['estimated_metrics']
        print(f"  {ratio}x å£“ç¸®:")
        print(f"    ç²¾åº¦æå¤±: {metrics['accuracy_loss_percent']}%")
        print(f"    åŠ é€Ÿ: {metrics['speedup']}")


def example_3_experiment_tracking():
    """ç¤ºä¾‹ 3: å¯¦é©—è¿½è¹¤å’Œåˆ†æ"""
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 3: å¯¦é©—è¿½è¹¤å’Œåˆ†æï¼ˆç„¡éœ€ API keyï¼‰")
    print("=" * 80)

    from src.agentic_compression.agents.tracking_tool import (
        LogExperimentTool,
        QueryExperimentsTool,
        GetBestConfigTool,
    )

    # è¨˜éŒ„å¹¾å€‹å¯¦é©—
    log_tool = LogExperimentTool()

    experiments = [
        {
            "config": {"technique": "quantization", "bits": 8, "method": "gptq"},
            "metrics": {"accuracy": 0.654, "latency_ms": 45.3, "memory_mb": 3421},
        },
        {
            "config": {"technique": "quantization", "bits": 4, "method": "gptq"},
            "metrics": {"accuracy": 0.612, "latency_ms": 28.1, "memory_mb": 1800},
        },
        {
            "config": {"technique": "pruning", "sparsity": 0.3, "pattern": "2:4"},
            "metrics": {"accuracy": 0.648, "latency_ms": 38.2, "memory_mb": 2900},
        },
    ]

    print("\nğŸ“ è¨˜éŒ„å¯¦é©—åˆ° MLflow:")
    for i, exp in enumerate(experiments, 1):
        result = log_tool._run(
            config=exp["config"],
            metrics=exp["metrics"],
            model_name="llama-2-7b",
            tags={"batch": "demo"},
        )
        print(f"  {i}. {result[:60]}...")

    # æŸ¥è©¢å¯¦é©—
    print(f"\nğŸ” æŸ¥è©¢é‡åŒ–å¯¦é©—:")
    query_tool = QueryExperimentsTool()
    result = query_tool._run(
        filter_string="params.technique = 'quantization'",
        max_results=5,
        order_by="metrics.accuracy DESC",
    )
    results = json.loads(result)
    for exp in results[:2]:  # é¡¯ç¤ºå‰ 2 å€‹
        print(f"  Bits: {exp['params'].get('bits', 'N/A')}")
        print(f"  Accuracy: {exp['metrics'].get('accuracy', 'N/A')}")
        print(f"  Latency: {exp['metrics'].get('latency_ms', 'N/A')} ms")

    # ç²å–æœ€ä½³é…ç½®
    print(f"\nğŸ† æœ€ä½³ç²¾åº¦é…ç½®:")
    best_tool = GetBestConfigTool()
    result = best_tool._run(
        metric="accuracy",
        higher_is_better=True,
    )
    best = json.loads(result)
    print(f"  é…ç½®: {best['best_config']}")
    print(f"  ç²¾åº¦: {best['metric_value']}")


def example_4_workspace_management():
    """ç¤ºä¾‹ 4: Workspace è¨˜æ†¶ç®¡ç†"""
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 4: Workspace è¨˜æ†¶ç®¡ç†ï¼ˆç„¡éœ€ API keyï¼‰")
    print("=" * 80)

    from src.agentic_compression.agents.compression_deep_agent import WorkspaceManager

    workspace = WorkspaceManager("./workspace")

    # ä¿å­˜å¯¦é©—
    print("\nğŸ’¾ ä¿å­˜å¯¦é©—åˆ° workspace:")
    workspace.save_experiment(
        experiment_id="exp_quant_8bit_001",
        config={"technique": "quantization", "bits": 8},
        metrics={"accuracy": 0.654, "speedup": 2.1},
    )
    print("  âœ“ å¯¦é©—å·²ä¿å­˜")

    # ä¿å­˜çŸ¥è­˜
    print("\nğŸ“š ä¿å­˜å­¸ç¿’åˆ°çš„çŸ¥è­˜:")
    workspace.save_knowledge(
        topic="quantization_best_practices",
        content="""# é‡åŒ–æœ€ä½³å¯¦è¸

## 8-bit é‡åŒ–
- ç²¾åº¦æå¤±: <1%
- åŠ é€Ÿ: 2-2.5x
- é©ç”¨å ´æ™¯: å¤§å¤šæ•¸éƒ¨ç½²

## 4-bit é‡åŒ–
- ç²¾åº¦æå¤±: 2-3%
- åŠ é€Ÿ: 3-4x
- é©ç”¨å ´æ™¯: é‚Šç·£è¨­å‚™ã€è³‡æºå—é™ç’°å¢ƒ
- å»ºè­°: ä½¿ç”¨ GPTQ æˆ– AWQ æ–¹æ³•

## æ³¨æ„äº‹é …
- å§‹çµ‚åœ¨ç›®æ¨™æ•¸æ“šé›†ä¸Šè©•ä¼°
- è€ƒæ…®æ ¡æº–æ•¸æ“šçš„è³ªé‡
- ç›£æ§é›¢ç¾¤å€¼çš„å½±éŸ¿
""",
    )
    print("  âœ“ çŸ¥è­˜å·²ä¿å­˜")

    # è®€å–çŸ¥è­˜
    print("\nğŸ“– è®€å–å·²ä¿å­˜çš„çŸ¥è­˜:")
    content = workspace.load_knowledge("quantization_best_practices")
    print(content[:200] + "...")


def example_5_comparison_report():
    """ç¤ºä¾‹ 5: ç”Ÿæˆå£“ç¸®æŠ€è¡“æ¯”è¼ƒå ±å‘Š"""
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 5: å£“ç¸®æŠ€è¡“æ¯”è¼ƒå ±å‘Šï¼ˆç„¡éœ€ API keyï¼‰")
    print("=" * 80)

    from src.agentic_compression.agents.sub_agents.lora_sub_agent import (
        EstimateLoRAImpactTool,
    )
    from src.agentic_compression.agents.sub_agents.distillation_sub_agent import (
        EstimateDistillationTool,
    )

    # æ¨¡å‹è¦æ ¼
    model = "meta-llama/Llama-2-7b-hf"

    # LoRA
    lora_tool = EstimateLoRAImpactTool()
    lora_result = json.loads(lora_tool._run(base_model=model, rank=8))

    # è’¸é¤¾
    distill_tool = EstimateDistillationTool()
    distill_result = json.loads(
        distill_tool._run(teacher_model=model, compression_ratio=2.0)
    )

    # é‡åŒ–ï¼ˆæ‰‹å‹•ä¼°ç®—ï¼‰
    quant_8bit = {
        "compression_ratio": 4.0,
        "accuracy_loss": 0.5,
        "speedup": "2.0x",
    }

    quant_4bit = {
        "compression_ratio": 8.0,
        "accuracy_loss": 2.0,
        "speedup": "3.0x",
    }

    print(f"\nğŸ“Š å£“ç¸®æŠ€è¡“æ¯”è¼ƒ - {model}\n")
    print("æŠ€è¡“                å£“ç¸®æ¯”    ç²¾åº¦æå¤±    åŠ é€Ÿ        é©ç”¨å ´æ™¯")
    print("-" * 80)

    print(
        f"{'8-bit é‡åŒ–':<15} {quant_8bit['compression_ratio']:>7.1f}x  "
        f"{quant_8bit['accuracy_loss']:>8.1f}%  {quant_8bit['speedup']:>8}  "
        f"é€šç”¨éƒ¨ç½²"
    )

    print(
        f"{'4-bit é‡åŒ–':<15} {quant_4bit['compression_ratio']:>7.1f}x  "
        f"{quant_4bit['accuracy_loss']:>8.1f}%  {quant_4bit['speedup']:>8}  "
        f"é‚Šç·£è¨­å‚™"
    )

    print(
        f"{'LoRA (rank=8)':<15} {lora_result['compression_ratio']:>7.1f}x  "
        f"{'<1.0':>8}%  {'1.0x':>8}  "
        f"ä»»å‹™å¾®èª¿"
    )

    distill_metrics = distill_result["estimated_metrics"]
    print(
        f"{'è’¸é¤¾ (2x)':<15} {distill_result['compression_ratio']:>7.1f}x  "
        f"{distill_metrics['accuracy_loss_percent']:>8.1f}%  {distill_metrics['speedup']:>8}  "
        f"æ¨¡å‹å£“ç¸®"
    )

    print("\nğŸ’¡ æ¨è–¦:")
    print("  â€¢ éœ€è¦é«˜ç²¾åº¦: 8-bit é‡åŒ–")
    print("  â€¢ ä»»å‹™ç‰¹å®šå„ªåŒ–: LoRA")
    print("  â€¢ æ¥µè‡´å£“ç¸®: è’¸é¤¾ + 4-bit é‡åŒ–")
    print("  â€¢ é‚Šç·£éƒ¨ç½²: 4-bit é‡åŒ–")


def main():
    """é‹è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("\n" + "=" * 80)
    print("ğŸ‰ Deep Agent å·¥å…·ä½¿ç”¨ç¤ºä¾‹ - ç„¡éœ€ API Key")
    print("=" * 80)

    example_1_lora_estimation()
    example_2_distillation_planning()
    example_3_experiment_tracking()
    example_4_workspace_management()
    example_5_comparison_report()

    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰ç¤ºä¾‹å®Œæˆï¼")
    print("\nğŸ’¡ æç¤º:")
    print("  â€¢ é€™äº›å·¥å…·éƒ½ä¸éœ€è¦ API key")
    print("  â€¢ å¯ä»¥ç›´æ¥åœ¨ä½ çš„ä»£ç¢¼ä¸­ä½¿ç”¨")
    print("  â€¢ æŸ¥çœ‹ MLflow UI: mlflow ui --backend-store-uri ./mlruns")
    print("  â€¢ æŸ¥çœ‹ workspace: ls -la workspace/")
    print("=" * 80 + "\n")


if __name__ == "__main__":
    main()
