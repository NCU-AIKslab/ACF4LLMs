{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b6abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # üßÆ Math 1.5B (A100-40GB) ‚Äî SFT ‚Üí RLVR ‚Üí PRM ‚Üí Repair ‚Üí DPO (Long‚ÜíShort) ‚Üí Length-Aware RL ‚Üí Eval\n",
    "# Auto-detects existing teacher data at /content/data/teacher_verified.jsonl and reuses it.\n",
    "# If not found, synthesizes Program-of-Thought traces via OpenAI teacher, verifies locally, then proceeds.\n",
    "\n",
    "# %%capture\n",
    "!pip install -U \"transformers>=4.43\" \"accelerate>=0.30\" \"trl>=0.9.6\" peft datasets bitsandbytes \\\n",
    "  \"flash-attn>=2.5.8\" sympy \"lighteval>=0.4.0\" --no-build-isolation\n",
    "!pip install -U openai\n",
    "\n",
    "import torch, platform, sys, os, json, random, re, time, textwrap, traceback, glob\n",
    "from pathlib import Path\n",
    "import sympy as sp\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available(), \"| Dev:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"Python:\", sys.version, \"| OS:\", platform.platform())\n",
    "\n",
    "# Paths\n",
    "OUT  = Path(\"/content/output\"); OUT.mkdir(parents=True, exist_ok=True)\n",
    "DATA = Path(\"/content/data\"); DATA.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ==== Config (edit as needed) ====\n",
    "MODEL_ID  = \"Qwen/Qwen2.5-Math-1.5B\"   # or \"Qwen/Qwen2.5-Math-1.5B-Instruct\"\n",
    "USE_INSTRUCT_CHAT_TEMPLATE = False     # True if using the Instruct chat template\n",
    "\n",
    "MAX_SEQ_LEN = 4096\n",
    "SFT_EPOCHS  = 1\n",
    "SFT_LR      = 2e-5\n",
    "\n",
    "GRPO_STEPS  = 2000\n",
    "GRPO_LR     = 1e-6\n",
    "GRPO_GROUP  = 4\n",
    "GRPO_KL     = 0.02\n",
    "\n",
    "SYNTH_SAMPLES = 2000                   # how many PoT examples to synthesize if needed\n",
    "TEACHER_MODEL = \"gpt-5\"                # replace with your org's model id\n",
    "TEMPERATURE   = 0.2\n",
    "\n",
    "EVAL_N = 200                           # quick eval size (increase for full test)\n",
    "USE_MATH = False                       # optional (clones official MATH repo)\n",
    "\n",
    "# OpenAI key prompt if not present\n",
    "if \"OPENAI_API_KEY\" not in os.environ or not os.environ[\"OPENAI_API_KEY\"]:\n",
    "    import getpass\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OPENAI_API_KEY (hidden): \")\n",
    "print(\"API key present:\", bool(os.environ.get(\"OPENAI_API_KEY\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d086323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## üìö Load eval/train datasets (GSM8K; optional MATH via GitHub)\n",
    "gsm = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "gsm_train = gsm[\"train\"]\n",
    "gsm_test  = gsm[\"test\"]\n",
    "print(\"GSM8K train/test:\", len(gsm_train), len(gsm_test))\n",
    "\n",
    "math_items = []\n",
    "if USE_MATH:\n",
    "    !rm -rf /content/math && git clone --depth 1 https://github.com/hendrycks/math /content/math\n",
    "    for fp in glob.glob(\"/content/math/train/**/*.json\", recursive=True):\n",
    "        try:\n",
    "            ex = json.load(open(fp))\n",
    "            prob, sol = ex.get(\"problem\",\"\"), ex.get(\"solution\",\"\")\n",
    "            m = re.search(r\"\\\\boxed\\{([^}]*)\\}\", sol) or re.search(r\"Answer:\\s*([^\\n]+)\", sol)\n",
    "            ans = m.group(1).strip() if m else None\n",
    "            if prob and ans:\n",
    "                math_items.append({\"question\": prob, \"answer\": ans})\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(\"Parsed MATH problems:\", len(math_items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf3a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## üßë‚Äçüè´ Teacher synthesis (PoT) ‚òÖ Auto-skip if /content/data/teacher_verified.jsonl exists\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "PROMPT = \"\"\"You are a math tutor. Solve the user's problem by:\n",
    "1) writing a short Python function `solve()` that computes the exact answer\n",
    "2) include 1-3 python assertions that verify the result (sympy ok: import sympy as sp)\n",
    "3) FINALLY print a single line 'Answer: <final>' where <final> is a plain number or simplified expression.\n",
    "\n",
    "Rules:\n",
    "- Keep code minimal/deterministic; allowed: math, fractions, decimal, itertools, sympy as sp\n",
    "- The printed 'Answer: <final>' must be exactly the final value (string)\n",
    "Return ONLY a JSON object with keys: cot_program (string), tests (list of strings), final_answer (string).\n",
    "\"\"\"\n",
    "\n",
    "def ask_teacher(question, model=TEACHER_MODEL, temperature=TEMPERATURE, use_responses_api=True):\n",
    "    if use_responses_api:\n",
    "        resp = client.responses.create(\n",
    "            model=model,\n",
    "            instructions=PROMPT,\n",
    "            input=[{\"role\":\"user\",\"content\":question}],\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        txt = resp.output_text\n",
    "    else:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model, temperature=temperature,\n",
    "            messages=[{\"role\":\"system\",\"content\":PROMPT},\n",
    "                      {\"role\":\"user\",\"content\":question}]\n",
    "        )\n",
    "        txt = resp.choices[0].message.content\n",
    "    m = re.search(r\"\\{.*\\}\", txt, flags=re.S)\n",
    "    if not m:\n",
    "        raise ValueError(\"No JSON found in teacher response\")\n",
    "    return json.loads(m.group(0))\n",
    "\n",
    "# Sandbox for PoT verification\n",
    "ALLOWED_BUILTINS = {\"abs\":abs, \"min\":min, \"max\":max, \"range\":range, \"len\":len, \"sum\":sum, \"print\":print}\n",
    "SAFE_GLOBALS = {\"__builtins__\": ALLOWED_BUILTINS, \"math\": __import__(\"math\"),\n",
    "                \"fractions\": __import__(\"fractions\"), \"decimal\": __import__(\"decimal\"),\n",
    "                \"itertools\": __import__(\"itertools\"), \"sp\": sp}\n",
    "\n",
    "def verify_record(rec):\n",
    "    code = rec[\"cot_program\"]; tests = rec.get(\"tests\", [])\n",
    "    final = rec[\"final_answer\"].strip()\n",
    "    loc = {}\n",
    "    try:\n",
    "        exec(code, SAFE_GLOBALS, loc)\n",
    "    except Exception as e:\n",
    "        return False, f\"exec error: {e}\"\n",
    "    for t in tests:\n",
    "        try:\n",
    "            exec(t, {**SAFE_GLOBALS, **loc}, {})\n",
    "        except Exception as e:\n",
    "            return False, f\"test fail: {e}\"\n",
    "    if \"solve\" in loc and callable(loc[\"solve\"]):\n",
    "        try:\n",
    "            got = loc[\"solve\"]()\n",
    "            if str(got).strip() != final:\n",
    "                try:\n",
    "                    if not sp.simplify(sp.nsimplify(got) - sp.nsimplify(final)) == 0:\n",
    "                        return False, f\"mismatch: solve()={got} vs final={final}\"\n",
    "                except Exception:\n",
    "                    return False, f\"mismatch: solve()={got} vs final={final}\"\n",
    "        except Exception as e:\n",
    "            return False, f\"solve() error: {e}\"\n",
    "    return True, \"ok\"\n",
    "\n",
    "verified_path = DATA / \"teacher_verified.jsonl\"\n",
    "raw_path      = DATA / \"teacher_raw.jsonl\"\n",
    "\n",
    "verified = []\n",
    "if verified_path.exists() and verified_path.stat().st_size > 0:\n",
    "    print(\"üü¢ Found existing verified teacher data:\", verified_path)\n",
    "    verified = [json.loads(l) for l in open(verified_path)]\n",
    "    print(\"Loaded verified records:\", len(verified))\n",
    "else:\n",
    "    print(\"üü° No verified teacher data found. Synthesizing now‚Ä¶\")\n",
    "    pool = [{\"id\": f\"gsm8k_{i}\", \"question\": ex[\"question\"], \"gold\": ex[\"answer\"]} for i, ex in enumerate(gsm_train)]\n",
    "    pool += [{\"id\": f\"math_{i}\", \"question\": ex[\"question\"], \"gold\": ex[\"answer\"]} for i, ex in enumerate(math_items)]\n",
    "    random.seed(1337); random.shuffle(pool)\n",
    "    sel = pool[:SYNTH_SAMPLES]\n",
    "\n",
    "    raw_out = raw_path.open(\"w\")\n",
    "    ok_out  = verified_path.open(\"w\")\n",
    "    ok_count = 0\n",
    "    for k, ex in enumerate(sel, 1):\n",
    "        try:\n",
    "            js = ask_teacher(ex[\"question\"], use_responses_api=True)\n",
    "            rec = {\n",
    "                \"id\": ex[\"id\"], \"question\": ex[\"question\"],\n",
    "                \"cot_program\": js[\"cot_program\"], \"tests\": js.get(\"tests\", []),\n",
    "                \"final_answer\": str(js[\"final_answer\"]).strip(), \"tool_mode\": \"python\"\n",
    "            }\n",
    "            print(json.dumps(rec), file=raw_out, flush=True)\n",
    "            ok, msg = verify_record(rec)\n",
    "            if ok:\n",
    "                ok_count += 1\n",
    "                verified.append(rec)\n",
    "                print(json.dumps(rec), file=ok_out, flush=True)\n",
    "            if k % 25 == 0:\n",
    "                print(f\"[{k}/{len(sel)}] verified_ok={ok_count}\")\n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "    raw_out.close(); ok_out.close()\n",
    "    print(\"Synthesized & verified:\", ok_count, \"of\", len(sel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e53c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## üß± Build SFT dataset from verified teacher data\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "\n",
    "def to_chat_text(q, prog, ans):\n",
    "    system = \"You are a concise math solver. First write minimal Python to compute the answer, then output 'Answer: <value>'.\"\n",
    "    if USE_INSTRUCT_CHAT_TEMPLATE and hasattr(tok, \"apply_chat_template\"):\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\":system},\n",
    "            {\"role\":\"user\",\"content\":q},\n",
    "            {\"role\":\"assistant\",\"content\":f\"# python\\n{prog}\\n\\nAnswer: {ans}\"}\n",
    "        ]\n",
    "        return tok.apply_chat_template(messages, tokenize=False)\n",
    "    else:\n",
    "        return f\"<|system|>\\n{system}\\n<|user|>\\n{q}\\n<|assistant|>\\n# python\\n{prog}\\n\\nAnswer: {ans}\"\n",
    "\n",
    "sft_path = DATA / \"sft_train.jsonl\"\n",
    "with sft_path.open(\"w\") as f:\n",
    "    for r in verified:\n",
    "        print(json.dumps({\"text\": to_chat_text(r[\"question\"], r[\"cot_program\"], r[\"final_answer\"])}), file=f)\n",
    "print(\"SFT records:\", sum(1 for _ in open(sft_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f847fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## üìò SFT (PoT) with TRL SFTTrainer\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "train_ds = load_dataset(\"json\", data_files=str(sft_path))[\"train\"]\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, torch_dtype=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "sft_cfg = SFTConfig(\n",
    "    output_dir=str(OUT / \"sft-poT\"),\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=SFT_LR,\n",
    "    num_train_epochs=SFT_EPOCHS,\n",
    "    bf16=True, logging_steps=10, save_steps=200,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model, tokenizer=tok, train_dataset=train_ds,\n",
    "    args=sft_cfg, dataset_text_field=\"text\", packing=False\n",
    ")\n",
    "sft_trainer.train()\n",
    "sft_trainer.save_model(str(OUT / \"sft-poT\" / \"final\"))\n",
    "print(\"‚úÖ SFT done ->\", OUT / \"sft-poT\" / \"final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa73f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## üß™ RL dataset (prompts + ground truth)\n",
    "def extract_numeric(a):\n",
    "    m = re.search(r\"[-+]?[0-9]*\\.?[0-9]+(?:/[0-9]+)?\", a)\n",
    "    return m.group(0) if m else a.strip()\n",
    "\n",
    "rl_list = [{\"prompt\": ex[\"question\"], \"ground_truth\": extract_numeric(ex[\"answer\"])} for ex in gsm_train]\n",
    "if len(math_items) > 0:\n",
    "    for ex in math_items[:2000]:\n",
    "        rl_list.append({\"prompt\": ex[\"question\"], \"ground_truth\": str(ex[\"answer\"])})\n",
    "\n",
    "random.shuffle(rl_list)\n",
    "rl_ds = Dataset.from_list(rl_list)\n",
    "print(\"RL dataset size:\", len(rl_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29031de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## üèÅ GRPO (verifiable reward): correctness + mild brevity shaping\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def parse_final(text:str):\n",
    "    m = re.search(r\"(?i)Answer:\\s*([^\\n]+)\", text)\n",
    "    if not m: m = re.search(r\"\\\\boxed\\{([^}]+)\\}\", text)\n",
    "    return m.group(1).strip() if m else text.strip().splitlines()[-1]\n",
    "\n",
    "def eq_correct(got, want):\n",
    "    try:\n",
    "        return sp.simplify(sp.nsimplify(got) - sp.nsimplify(want)) == 0\n",
    "    except Exception:\n",
    "        return str(got).strip() == str(want).strip()\n",
    "\n",
    "# In the math_reward_func, add PRM scoring:\n",
    "def math_reward_func(prompts, completions, ground_truth, **kwargs):\n",
    "    rewards = []\n",
    "    for prompt, comp, gt in zip(prompts, completions, ground_truth):\n",
    "        content = comp if isinstance(comp, str) else comp[0][\"content\"]\n",
    "        try:\n",
    "            pred = parse_final(content)\n",
    "            correct = eq_correct(pred, gt)\n",
    "            \n",
    "            # ADD THESE 2 LINES: Include PRM score in reward\n",
    "            lines = [ln.strip() for ln in content.splitlines() if ln.strip()]\n",
    "            prm_bonus = sum(get_prm_score(prompt, ln, i+1) for i, ln in enumerate(lines[:5])) / max(1, len(lines[:5])) * 0.1\n",
    "            \n",
    "            r = (1.0 if correct else 0.0) - 0.0002 * len(content) + prm_bonus  # MODIFY THIS LINE\n",
    "            rewards.append(float(r))\n",
    "        except Exception:\n",
    "            rewards.append(-0.1)\n",
    "    return rewards\n",
    "\n",
    "policy = AutoModelForCausalLM.from_pretrained(\n",
    "    str(OUT / \"sft-poT\" / \"final\"),\n",
    "    torch_dtype=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "grpo_cfg = GRPOConfig(\n",
    "    output_dir=str(OUT / \"grpo\"),\n",
    "    learning_rate=GRPO_LR,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    bf16=True, logging_steps=10, save_steps=100,\n",
    "    max_prompt_length=1024, max_completion_length=512,\n",
    "    num_generations=GRPO_GROUP, kl_coeff=GRPO_KL\n",
    ")\n",
    "\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    model=policy,\n",
    "    reward_funcs=math_reward_func,\n",
    "    train_dataset=rl_ds.select(range(min(len(rl_ds), 6000))),\n",
    "    processing_class=tok, args=grpo_cfg\n",
    ")\n",
    "grpo_trainer.train(max_steps=GRPO_STEPS)\n",
    "grpo_trainer.save_model(str(OUT / \"grpo\" / \"final\"))\n",
    "print(\"‚úÖ GRPO done ->\", OUT / \"grpo\" / \"final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b3314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## üß≠ PRM (Process Reward Model) ‚Äî simple heuristic step labels from verified traces\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "def build_prm_dataset(verified_records):\n",
    "    samples = []\n",
    "    for r in verified_records:\n",
    "        code = r[\"cot_program\"]\n",
    "        lines = [ln for ln in code.splitlines() if ln.strip()]\n",
    "        for i, ln in enumerate(lines):\n",
    "            # Heuristic: verified solution -> mark earlier lines as correct (1)\n",
    "            label = 1\n",
    "            txt = f\"{r['question']}\\n\\n# step {i+1}\\n{ln}\"\n",
    "            samples.append({\"text\": txt, \"label\": label})\n",
    "    return Dataset.from_list(samples)\n",
    "\n",
    "# Reload verified from disk (robust to fresh runtime)\n",
    "verified = [json.loads(l) for l in open(DATA/\"teacher_verified.jsonl\")]\n",
    "prm_train_ds = build_prm_dataset(verified)\n",
    "print(\"PRM samples:\", len(prm_train_ds))\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tok2 = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "\n",
    "def tokenize_prm(ex):\n",
    "    return tok2(ex[\"text\"], truncation=True, max_length=1024)\n",
    "prm_ds = prm_train_ds.map(tokenize_prm)\n",
    "\n",
    "prm_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_ID, num_labels=2, torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "prm_args = TrainingArguments(\n",
    "    output_dir=str(OUT / \"prm\"),\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    bf16=True, logging_steps=20, save_steps=200\n",
    ")\n",
    "prm_trainer = Trainer(\n",
    "    model=prm_model, args=prm_args,\n",
    "    train_dataset=prm_ds, tokenizer=tok2\n",
    ")\n",
    "prm_trainer.train()\n",
    "prm_trainer.save_model(str(OUT / \"prm\" / \"final\"))\n",
    "print(\"‚úÖ PRM done ->\", OUT / \"prm\" / \"final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec81857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## üîÅ StepCo-style Verify-Then-Revise helper\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "final_model_path = str(OUT / \"grpo\" / \"final\")\n",
    "eval_model = AutoModelForCausalLM.from_pretrained(\n",
    "    final_model_path, torch_dtype=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\", device_map=\"auto\"\n",
    ")\n",
    "eval_tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if eval_tok.pad_token is None: eval_tok.pad_token = eval_tok.eos_token\n",
    "\n",
    "def generate_reply(model, tok, prompt, temperature=0.0, max_new_tokens=512):\n",
    "    if USE_INSTRUCT_CHAT_TEMPLATE and hasattr(tok, \"apply_chat_template\"):\n",
    "        messages = [\n",
    "            {\"role\":\"system\",\"content\":\"You are a concise math solver. Write minimal Python, then 'Answer: <value>'.\"},\n",
    "            {\"role\":\"user\",\"content\":prompt}\n",
    "        ]\n",
    "        full = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    else:\n",
    "        full = f\"<|system|>\\nYou are a concise math solver. Write minimal Python, then 'Answer: <value>'.\\n<|user|>\\n{prompt}\\n<|assistant|>\\n\"\n",
    "    inputs = tok([full], return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(**inputs, do_sample=temperature>0, temperature=temperature, max_new_tokens=max_new_tokens)[0]\n",
    "    text = tok.decode(out[inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "def solve_with_pot(question, model=eval_model, tok=eval_tok):\n",
    "    text = generate_reply(model, tok, question, temperature=0.0)\n",
    "    rec = {\"cot_program\": text, \"tests\": [], \"final_answer\": parse_final(text)}\n",
    "    ok, _ = verify_record(rec)\n",
    "    return text, rec[\"final_answer\"], ok\n",
    "\n",
    "def stepco_repair(question, max_rounds=2):\n",
    "    text, pred, ok = solve_with_pot(question)\n",
    "    if ok: return text, pred, True\n",
    "    for _ in range(max_rounds):\n",
    "        repair_prompt = f\"The following solution seems incorrect. Fix only the wrong steps and keep it concise.\\n\\nQuestion:\\n{question}\\n\\nSolution:\\n{text}\"\n",
    "        text = generate_reply(eval_model, eval_tok, repair_prompt, temperature=0.0)\n",
    "        rec = {\"cot_program\": text, \"tests\": [], \"final_answer\": parse_final(text)}\n",
    "        ok, _ = verify_record(rec)\n",
    "        if ok:\n",
    "            return text, rec[\"final_answer\"], True\n",
    "    return text, parse_final(text), False\n",
    "\n",
    "print(\"Repair helper ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e17d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## üîÑ PRM-Guided Self-Evolution (A6): Best-of-N + shallow MCTS for better training data\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load trained PRM\n",
    "prm_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    str(OUT / \"prm\" / \"final\"), torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "prm_tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "\n",
    "def get_prm_score(question, code_line, step_num):\n",
    "    \"\"\"Get PRM confidence score for a code step\"\"\"\n",
    "    text = f\"{question}\\n\\n# step {step_num}\\n{code_line}\"\n",
    "    inputs = prm_tok(text, return_tensors=\"pt\", truncation=True, max_length=1024).to(prm_model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = prm_model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return float(probs[0][1])  # probability of \"correct\" label\n",
    "\n",
    "def best_of_n_with_prm(question, n=4, model=eval_model, tok=eval_tok):\n",
    "    \"\"\"Generate N solutions and pick best by PRM scores\"\"\"\n",
    "    candidates = []\n",
    "    for _ in range(n):\n",
    "        text = generate_reply(model, tok, question, temperature=0.3, max_new_tokens=512)\n",
    "        \n",
    "        # Calculate average PRM score across code steps\n",
    "        lines = [ln.strip() for ln in text.splitlines() if ln.strip() and not ln.strip().startswith('#')]\n",
    "        if not lines:\n",
    "            candidates.append((text, 0.0))\n",
    "            continue\n",
    "            \n",
    "        scores = []\n",
    "        for i, line in enumerate(lines[:10]):  # limit to first 10 steps\n",
    "            try:\n",
    "                score = get_prm_score(question, line, i+1)\n",
    "                scores.append(score)\n",
    "            except:\n",
    "                scores.append(0.5)  # neutral score on error\n",
    "        \n",
    "        avg_score = sum(scores) / len(scores) if scores else 0.0\n",
    "        candidates.append((text, avg_score))\n",
    "    \n",
    "    # Return best candidate\n",
    "    best_text, best_score = max(candidates, key=lambda x: x[1])\n",
    "    return best_text, best_score\n",
    "\n",
    "# Generate enhanced training data using PRM guidance\n",
    "print(\"üîÑ Generating PRM-guided enhanced training data...\")\n",
    "enhanced_records = []\n",
    "\n",
    "# Use subset of original GSM8K for self-evolution\n",
    "evolution_samples = gsm_train.select(range(min(800, len(gsm_train))))\n",
    "\n",
    "for i, ex in enumerate(evolution_samples):\n",
    "    try:\n",
    "        # Generate best-of-N solution guided by PRM\n",
    "        enhanced_text, prm_score = best_of_n_with_prm(ex[\"question\"])\n",
    "        \n",
    "        # Verify the enhanced solution\n",
    "        rec = {\n",
    "            \"cot_program\": enhanced_text, \n",
    "            \"tests\": [], \n",
    "            \"final_answer\": parse_final(enhanced_text)\n",
    "        }\n",
    "        is_correct, _ = verify_record(rec)\n",
    "        \n",
    "        # Only keep if both PRM likes it AND it's actually correct\n",
    "        if is_correct and prm_score > 0.6:  # threshold for quality\n",
    "            enhanced_records.append({\n",
    "                \"question\": ex[\"question\"],\n",
    "                \"cot_program\": enhanced_text,\n",
    "                \"final_answer\": rec[\"final_answer\"],\n",
    "                \"prm_score\": prm_score\n",
    "            })\n",
    "            \n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f\"[{i+1}/{len(evolution_samples)}] enhanced_collected={len(enhanced_records)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        if i < 5:  # only print first few errors\n",
    "            print(f\"Error on sample {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"‚úÖ Generated {len(enhanced_records)} PRM-enhanced training examples\")\n",
    "\n",
    "# Save enhanced data for potential SFT fine-tuning\n",
    "enhanced_path = DATA / \"prm_enhanced.jsonl\"\n",
    "with enhanced_path.open(\"w\") as f:\n",
    "    for rec in enhanced_records:\n",
    "        print(json.dumps(rec), file=f)\n",
    "\n",
    "print(f\"Saved enhanced data to: {enhanced_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## ‚úÇÔ∏è Long‚ÜíShort preference data & DPO training\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def make_short_from_program(code:str, final_ans:str):\n",
    "    kept = []\n",
    "    for ln in code.splitlines():\n",
    "        s = ln.strip()\n",
    "        if s.startswith(\"#\"): \n",
    "            continue\n",
    "        if \"print(\" in s and \"Answer:\" not in s:\n",
    "            continue\n",
    "        kept.append(ln)\n",
    "    short = \"\\n\".join(kept)\n",
    "    return f\"{short}\\n\\nAnswer: {final_ans}\"\n",
    "\n",
    "pref_recs = []\n",
    "for r in verified[: min(1500, len(verified))]:\n",
    "    q = r[\"question\"]\n",
    "    long = f\"{r['cot_program']}\\n\\nAnswer: {r['final_answer']}\"\n",
    "    short = make_short_from_program(r[\"cot_program\"], r[\"final_answer\"])\n",
    "    pref_recs.append({\"prompt\": q, \"chosen\": short, \"rejected\": long})\n",
    "\n",
    "pref_path = DATA / \"short_vs_long.jsonl\"\n",
    "with pref_path.open(\"w\") as f:\n",
    "    for ex in pref_recs:\n",
    "        print(json.dumps(ex), file=f)\n",
    "\n",
    "pref_ds = load_dataset(\"json\", data_files=str(pref_path))[\"train\"]\n",
    "\n",
    "dpo_model = AutoModelForCausalLM.from_pretrained(\n",
    "    str(OUT / \"grpo\" / \"final\"),\n",
    "    torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "dpo_cfg = DPOConfig(\n",
    "    output_dir=str(OUT / \"dpo\"),\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-6,\n",
    "    bf16=True, logging_steps=20, save_steps=200,\n",
    "    max_length=1024\n",
    ")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=dpo_model, ref_model=None, tokenizer=eval_tok,\n",
    "    args=dpo_cfg, train_dataset=pref_ds\n",
    ")\n",
    "dpo_trainer.train()\n",
    "dpo_trainer.save_model(str(OUT / \"dpo\" / \"final\"))\n",
    "print(\"‚úÖ DPO done ->\", OUT / \"dpo\" / \"final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c08a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## üß† Length-aware RL (optional but included): stronger brevity penalty after DPO\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "\n",
    "def lapo_like_reward(prompts, completions, ground_truth, **kwargs):\n",
    "    rewards = []\n",
    "    for comp, gt in zip(completions, ground_truth):\n",
    "        content = comp if isinstance(comp, str) else comp[0][\"content\"]\n",
    "        try:\n",
    "            pred = parse_final(content)\n",
    "            correct = eq_correct(pred, gt)\n",
    "            r = (1.0 if correct else 0.0) - 0.0005 * len(content)  # stronger after DPO\n",
    "            rewards.append(float(r))\n",
    "        except Exception:\n",
    "            rewards.append(-0.1)\n",
    "    return rewards\n",
    "\n",
    "lapo_model = AutoModelForCausalLM.from_pretrained(\n",
    "    str(OUT / \"dpo\" / \"final\"),\n",
    "    torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "lapo_cfg = GRPOConfig(\n",
    "    output_dir=str(OUT / \"grpo-lapo\"),\n",
    "    learning_rate=1e-6, per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8, bf16=True,\n",
    "    logging_steps=10, save_steps=100,\n",
    "    max_prompt_length=1024, max_completion_length=512,\n",
    "    num_generations=GRPO_GROUP, kl_coeff=GRPO_KL\n",
    ")\n",
    "\n",
    "lapo_trainer = GRPOTrainer(\n",
    "    model=lapo_model, reward_funcs=lapo_like_reward,\n",
    "    train_dataset=rl_ds.select(range(min(len(rl_ds), 4000))),\n",
    "    processing_class=eval_tok, args=lapo_cfg\n",
    ")\n",
    "lapo_trainer.train(max_steps=1000)\n",
    "lapo_trainer.save_model(str(OUT / \"grpo-lapo\" / \"final\"))\n",
    "print(\"‚úÖ Length-aware RL done ->\", OUT / \"grpo-lapo\" / \"final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd09940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## ‚úÖ Evaluation (GSM8K quick pass@1 with PoT execution)\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "final_eval_path = str(OUT / \"grpo-lapo\" / \"final\") if (OUT / \"grpo-lapo\" / \"final\").exists() else str(OUT / \"dpo\" / \"final\")\n",
    "eval_model = AutoModelForCausalLM.from_pretrained(\n",
    "    final_eval_path, torch_dtype=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "def evaluate_gsm8k(n=EVAL_N):\n",
    "    subset = gsm_test.select(range(min(n, len(gsm_test))))\n",
    "    correct = 0\n",
    "    for i, ex in enumerate(subset):\n",
    "        text = generate_reply(eval_model, eval_tok, ex[\"question\"], temperature=0.0, max_new_tokens=512)\n",
    "        pred = parse_final(text)\n",
    "        gold = extract_numeric(ex[\"answer\"])\n",
    "        good = eq_correct(pred, gold)\n",
    "        if not good:\n",
    "            # try one repair round for stubborn cases (cheap)\n",
    "            _, pred2, ok2 = stepco_repair(ex[\"question\"], max_rounds=1)\n",
    "            good = ok2 and eq_correct(pred2, gold)\n",
    "        correct += int(good)\n",
    "        if (i+1) % 25 == 0:\n",
    "            print(f\"[{i+1}/{len(subset)}] acc={correct/(i+1):.3f}\")\n",
    "    print(f\"Final GSM8K@{len(subset)}: acc={correct/len(subset):.3f}\")\n",
    "\n",
    "evaluate_gsm8k(EVAL_N)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
